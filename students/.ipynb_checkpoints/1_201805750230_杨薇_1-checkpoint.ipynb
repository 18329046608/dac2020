{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名+作业序号，示例：``1_20188989899_张三_1``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "utf-8\n",
      "<!DOCTYPE html>\r\n",
      "<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus=autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=百度一下 class=\"bg s_btn\" autofocus></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>新闻</a> <a href=https://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>地图</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>视频</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>贴吧</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>登录</a> </noscript> <script>document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">登录</a>');\r\n",
      "                </script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">更多产品</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>关于百度</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>使用百度前必读</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>意见反馈</a>&nbsp;京ICP证030173号&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests  # 导出requests\n",
    "r = requests.get(\"https://www.baidu.com\") # 用get方法访问百度\n",
    "print(r.status_code)  # 查看网页状态码，如果是200则继续\n",
    "r.encoding = r.apparent_encoding  # 修改响应内容编码方式\n",
    "print(r.encoding)\n",
    "print(r.text)  # 打印response文本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析：\n",
    "+ 由打印出来的html内容可以看到有`head`部分和`body`部分\n",
    "+ 其中`head`部分可以看到很多基本的参数\n",
    "+ `body`部分有很多的网页超链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests  # 导出requests\n",
    "kv = {'user-agent':'Mozilla/5.0'}  # 伪装成火狐浏览器为了访问亚马逊\n",
    "r = requests.post(\"https://www.amazon.cn/gp/product/B01M8L5z3Y\",headers = kv)  # 用post方法访问亚马逊\n",
    "print(r.status_code)  # 查看网页状态码，如果是200则继续\n",
    "r.encoding = r.apparent_encoding  # 修改响应内容编码方式\n",
    "print(r.text)  # 打印response文本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由输出的信息可以看出亚马逊是一个大站，网站上面有很多的页面信息，非常的复杂，信息非常的丰富。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cookies': {'Yang': '12345678'}}\n"
     ]
    }
   ],
   "source": [
    "import requests  # 导入requests库\n",
    "import json\n",
    "s = requests.Session()\n",
    "s.get('http://httpbin.org/cookies/set/Yang/12345678')# 用session对象发出另外一个get请求，获取cookies \n",
    "r = s.get(\"http://httpbin.org/cookies\") # 显示设置之后的结果 \n",
    "print(json.loads(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以字典形式返回了设置的cookies内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests, re\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'https://www.hao123.com/' # 这是一个神奇的网站，专门为爬虫练习而生\n",
    "queue = [seed]\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = queue.pop(-1)\n",
    "        html = requests.get(url,timeout=5).text\n",
    "        new_urls = r.findall(html)\n",
    "        queue.extend(new_urls)\n",
    "        storage[url] = len(new_urls)\n",
    "        for new_url in new_urls:\n",
    "            print(new_url)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析：从输出结果可以看出深度爬取出现了很多重复的URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不去重\n",
    "import requests, re\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'https://www.hao123.com/' # 这是一个神奇的网站，专门为爬虫练习而生\n",
    "queue = [seed]\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = queue.pop(0)  # 广度遍历\n",
    "        html = requests.get(url,timeout=5).text  #用get 方法发出请求\n",
    "        new_urls = r.findall(html)\n",
    "        queue.extend(new_urls)\n",
    "        storage[url] = len(new_urls)\n",
    "        for new_url in new_urls:\n",
    "            print(new_url)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dgss2.bdstatic.com 1\n",
      "https://dgss3.bdstatic.com 1\n",
      "https://s.click.taobao.com/LkFcknv 1\n",
      "https://pan.baidu.com/ 1\n",
      "http://app.hao123.com/app 1\n",
      "http://dl.hao123.com/waphao123/n_apk/hao1231019956s.apk 1\n",
      "https://ccc-x.jd.com/dsp/cl?posid=1999&amp;v=707&amp;union_id=1000023384&amp;pid=2147&amp;tagid=125063&amp;didmd5=__IMEI__&amp;idfamd5=__IDFA__&amp;did=__IMEIIMEI__&amp;idfa=__IDFAIDFA__&amp;to=https://miaosha.jd.com/ 1\n",
      "https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_pc_4 1\n",
      "http://www.hao123.com/redian/sheshouyef.htm 1\n",
      "http://www.baidu.com/s?word=%E6%B9%96%E5%8D%97%E9%95%BF%E6%B2%99%E4%B8%80%E5%91%A8%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5&tn=50000167_hao_pg&ie=utf-8 1\n"
     ]
    }
   ],
   "source": [
    "# 去重\n",
    "import requests, re\n",
    "count = 3\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'https://www.hao123.com/' # 爬取hao123网站信息\n",
    "queue = [seed]\n",
    "used = set() # 设置一个集合，保存已经抓取过的 URL\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0: \n",
    "    try:\n",
    "        url = queue.pop(0)   # 广度遍历\n",
    "        html = requests.get(url).text\n",
    "        storage[url] = html\n",
    "        used.add(url)\n",
    "        new_urls = r.findall(html)\n",
    "        for new_url in new_urls:     # for遍历爬取到的所有URL\n",
    "            if new_url not in used and new_url not in queue:\n",
    "                queue.append(new_url)   # 将没有遍历过的URL添加到集合中\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)\n",
    "from collections import Counter  \n",
    "url_count = Counter(queue)\n",
    "for url,count in url_count.most_common(10):\n",
    "    print(url,count)  #打印去重后的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
