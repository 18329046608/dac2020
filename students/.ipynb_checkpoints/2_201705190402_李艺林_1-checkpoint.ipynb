{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.request``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名，示例：``1_20188989899_张三``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.request``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "import urllib.request    #导入urllib.request  \n",
    "# 对百度百科无心法师为网络爬虫的URL的进行访问\n",
    "response = urllib.request.urlopen('https://baike.baidu.com/item/2020%E5%B9%B4%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E%E7%96%AB%E6%83%85%E5%8F%91%E5%B1%95%E5%AE%9E%E5%BD%95/24334213?fr=aladdin').read().decode('utf-8')\n",
    "print(response)#输出文本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与预测结果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "from urllib import request,parse  #导入requests\n",
    "url=\"https://baike.baidu.com/item/2020%E5%B9%B4%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E%E7%96%AB%E6%83%85%E5%8F%91%E5%B1%95%E5%AE%9E%E5%BD%95/24334213?fr=aladdin\"  #url，必须有的参数，其余是可选参数\n",
    "headers={\n",
    "    #伪装一个火狐浏览器\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; U; Linux i686)Gecko/20071127 Firefox/2.0.0.11\", #设置为火狐浏览器\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8\"\n",
    "\n",
    "}\n",
    "dict={\n",
    "    \"name\":\"michael\"\n",
    "}\n",
    "data=bytes(parse.urlencode(dict),encoding=\"utf8\")\n",
    "req=request.Request(url=url,data=data,headers=headers,method=\"POST\")     #method指示请求使用post\n",
    "response=request.urlopen(req).read().decode(\"utf-8\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与预测结果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"cookies\": {\\n    \"sessioncookie\": \"123456789\"\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里编写代码\n",
    "import http.cookiejar,urllib.request\n",
    "#创建一个session对象\n",
    "s=requests.Session()\n",
    "#用session对象发出get请求，设置cookies\n",
    "s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')\n",
    "#用session对象发出另外一个get请求，获取cookies\n",
    "r=s.get(\"http://httpbin.org/cookies\")\n",
    "#显示结果\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这道题不理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "#深度优先\n",
    "import urllib.request as ur\n",
    "import re\n",
    "count = 0\n",
    "r = re.compile(r'href=[\\'\"]?（/item[^\\'\">]）+）')  #正则表达式\n",
    "seed = \"/item/2020%E5%B9%B4%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E%E7%96%AB%E6%83%85%E5%8F%91%E5%B1%95%E5%AE%9E%E5%BD%95\"  #网络爬虫词条\n",
    "queue = [seed]  #设置种子链接的栈\n",
    "storage ={}\n",
    "while len(queue)>0:\n",
    "    try:\n",
    "        url = queue.pop(-1)  #取出栈的最后一条URL\n",
    "        html = ur.urlopen(\"https://baike.baidu.com\"+url).read().decode('utf-8') # 对URL进行拼接\n",
    "        print(html)\n",
    "        new_urls=r.findall(html)\n",
    "        queue.extend(new_urls)    #将新提取的链接信息入队列\n",
    "        storage[url]=len(new_urls)\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度优先不去重，所以结果很多重复内容，与预测一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#广度优先爬取（不去重）\n",
    "import urllib.request as ur\n",
    "import re\n",
    "count = 0\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)') # 抽取所需链接信息的正则语言规则\n",
    "seed = \"/item/2020%E5%B9%B4%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E%E7%96%AB%E6%83%85%E5%8F%91%E5%B1%95%E5%AE%9E%E5%BD%95\" # 这是网络爬虫词条\n",
    "queue = [seed] # 设置种子链接的队列（使用列表模拟队列）\n",
    "storage = {}\n",
    "while count < 8:\n",
    "    try:\n",
    "        url = queue.pop(0) # 取出队列最后一条URL\n",
    "        html = ur.urlopen(\"https://baike.baidu.com\"+url).read().decode('utf-8') # 对URL进行拼接\n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)\n",
    "        queue.extend(new_urls) # 将新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/2020%E5%B9%B4%E6%96%B0%E5%9E%8B%E5%86%A0%E7%8A%B6%E7%97%85%E6%AF%92%E7%96%AB%E6%83%85/24278151', '/item/%E6%AD%A6%E6%B1%89%E5%B8%82/195165', '/item/2019%E6%96%B0%E5%9E%8B%E5%86%A0%E7%8A%B6%E7%97%85%E6%AF%92/24267858', '/item/2019%E5%B9%B4%E6%AD%A6%E6%B1%89%E7%97%85%E6%AF%92%E6%80%A7%E8%82%BA%E7%82%8E%E7%97%85%E4%BE%8B/24236082', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n",
    "/item/秒懂星课堂\n",
    "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
    "/item/秒懂大师说\n",
    "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
    "/item/秒懂看瓦特\n",
    "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
    "/item/秒懂五千年\n",
    "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
    "/item/秒懂全视界\n",
    "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
    "这里的内容不知道怎么运行出来的，不太明白。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/item/2020%E5%B9%B4%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E%E7%96%AB%E6%83%85%E5%8F%91%E5%B1%95%E5%AE%9E%E5%BD%95 0\n",
      "/item/秒懂全视界 1\n",
      "/item/秒懂全视界\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂看瓦特 1\n",
      "/item/秒懂看瓦特\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂五千年 1\n",
      "/item/秒懂五千年\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂星课堂 1\n",
      "/item/秒懂星课堂\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/2020%E5%B9%B4%E6%96%B0%E5%9E%8B%E5%86%A0%E7%8A%B6%E7%97%85%E6%AF%92%E7%96%AB%E6%83%85/24278151 1\n",
      "/item/2019%E5%B9%B4%E6%AD%A6%E6%B1%89%E7%97%85%E6%AF%92%E6%80%A7%E8%82%BA%E7%82%8E%E7%97%85%E4%BE%8B/24236082 1\n",
      "/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter 1\n",
      "/item/秒懂大师说 1\n",
      "/item/秒懂大师说\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/2019%E6%96%B0%E5%9E%8B%E5%86%A0%E7%8A%B6%E7%97%85%E6%AF%92/24267858 1\n",
      "/item/%E6%AD%A6%E6%B1%89%E5%B8%82/195165 1\n"
     ]
    }
   ],
   "source": [
    "#加层数控制的\n",
    "import urllib.request as ur\n",
    "import re\n",
    "count = 0 # 层数\n",
    "floors = 1 # 限制爬取的层数\n",
    "lastStep = []\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)')\n",
    "seed = '/item/2020%E5%B9%B4%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E%E7%96%AB%E6%83%85%E5%8F%91%E5%B1%95%E5%AE%9E%E5%BD%95' # 这是网络爬虫词条\n",
    "queue = [[seed]]\n",
    "for i in range(floors): # 限制爬取范围在设定的层数范围内\n",
    "    queue.append([])\n",
    "storage = {}\n",
    "used = set() # 设置集合存放爬取过的url\n",
    "while len(queue[0])>0 or count!=0: # 种子队列不为空或者层数不为零\n",
    "    try:\n",
    "        url = queue[count].pop(-1)\n",
    "        print(url+\" \"+str(count)) # 打印当前链接和层数\n",
    "        html = ur.urlopen('https://baike.baidu.com'+url).read().decode('utf-8')\n",
    "        storage[url]=html\n",
    "        used.add(url) # 将爬取过的URL放入集合中\n",
    "        new_urls = r.findall(html)\n",
    "        if count < floors:\n",
    "            for new_url in set(new_urls):\n",
    "                if new_url not in used and new_url not in queue:#判断新链接网址中的包含的链接是否为重复的\n",
    "                    queue[count+1].append(new_url) # 将爬取的URL存入到队列中相应层数的列表\n",
    "            count+=1\n",
    "        else:\n",
    "            if len(queue[count])==0:\n",
    "                count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "去重后无重复内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
