{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名+作业序号，示例：``1_20188989899_张三_1``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #导入requests库\n",
    "r = requests.get(\"https://www.op.gg\")#发出get请求\n",
    "print(r)#输出response\n",
    "print(r.headers)#输出头部信息\n",
    "print(r.text)#输出response文本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.使用get函数向网站发送请求，返回状态码200，表示请求成功!\n",
    "2.输出请求头部信息，各类信息如下：\n",
    "{Server': 'nginx/1.12.2', 'Date': 'Sat, 07 Mar 2020 02:25:45 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Transfer-Encoding': 'chunked', 'O-Host': '8f64f2100b5a', 'Content-Language': 'en-US', 'Content-Encoding': 'gzip', 'O-Host2': '39', 'Strict-Transport-Security': 'max-age=16000000; includeSubDomains; preload}\n",
    "3.response文本信息输出成功，网页源码得见。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}#火狐浏览器headers信息\n",
    "headers1 = {'User-Agent' :'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36 Edg/80.0.361.66'}#本机浏览器信息\n",
    "url = \"https://www.op.gg\" \n",
    "r = requests.post(url, headers= headers)#伪装成火狐浏览器发起一次post请求\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取到的response文本信息与用get方法获取到的一样，代码相较get方法来说长了一点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"cookies\": {\\n    \"sessioncookie\": \"261826182618zzzzzzrrrrrr201805750311tsoe\"\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = requests.Session()# 创建一个session对象\n",
    "\n",
    "s.get('http://httpbin.org/cookies/set/sessioncookie/261826182618zzzzzzrrrrrr201805750311tsoe')# 用session对象发出get请求，设置cookies\n",
    "r = s.get(\"http://httpbin.org/cookies\")# 用session对象发出另外一个get请求，获取cookies\n",
    "r.text\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成要求，cookies内容略长，读取时间过慢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request as ur\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "r = ur.urlopen(\"https://cn.bing.com/search?q=bing&cvid=077c2aeecd9e4fd0972d68f9cb5d36ac&FORM=ANNTA1&PC=U531/op.gg\")\n",
    "content = r.read().decode('utf-8')\n",
    "href = re.compile(r'href=[\\'\"]?(/[^\\'\" >]+)') # 利用正则表达式将网页中所需的链接表达出来\n",
    "new_urls=href.findall(content)\n",
    "\n",
    "#实现深度优先爬取\n",
    "count = 0\n",
    "\n",
    "r = re.compile(r'href=[\\'\"]?(/[^\\'\" >]+)') # 抽取所需链接信息的正则语言规则\n",
    "stack = [new_urls] # 将所获取的链接存入栈中\n",
    "storage = {}\n",
    "while count < 1:  #设置深度\n",
    "    try:\n",
    "        url = stack.pop(-1) # 取出栈的最后一条URL\n",
    "        html = ur.urlopen(url).read().decode('utf-8') \n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)#输出当前网页下的链接\n",
    "        stack.extend(new_urls) # 将新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取目标网页的url，将其入栈，再按照深度优先遍历去遍历网页。获取的数据略大，jupyter notebook易崩。在编写的时候，深度的设置也很重要，最后由于电脑限制未能完成对网站的遍历。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request as ur\n",
    "import re\n",
    "import time\n",
    "r = ur.urlopen(\"https://cn.bing.com\")\n",
    "content = r.read().decode('utf-8')\n",
    "href = re.compile(r'href=[\\'\"]?(/[^\\'\" >]+)')\n",
    "new_urls=href.findall(content)\n",
    "count = 0 # 设置广度\n",
    "queue = [new_urls] # \n",
    "storage = {}\n",
    "while count < 8:\n",
    "    try:\n",
    "        url = queue.pop(0) # 取出队列最后一条URL\n",
    "        html = ur.urlopen(\"https://cn.bing.com\"+url).read().decode('utf-8') # 对URL进行拼接\n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)\n",
    "        queue.extend(new_urls) # 将新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/search?format=rss&amp;q=bing&amp;cvid=077c2aeecd9e4fd0972d68f9cb5d36ac&amp;FORM=ANNTA1&amp;PC=U531/op.gg', '/search?format=rss&amp;q=bing&amp;cvid=077c2aeecd9e4fd0972d68f9cb5d36ac&amp;FORM=ANNTA1&amp;PC=U531/op.gg', '/sa/simg/bing_p_rr_teal_min.ico', '/?FORM=Z9FD1', '/?scope=web&amp;FORM=HDRSC1', '/images/search?q=bing&amp;FORM=HDRSC2', '/videos/search?q=bing&amp;FORM=HDRSC3', '/academic/search?q=bing&amp;FORM=HDRSC4', '/dict/search?q=bing&amp;FORM=HDRSC6', '/maps?q=bing&amp;FORM=HDRSC7', '/search?q=bing&amp;cvid=077c2aeecd9e4fd0972d68f9cb5d36ac&amp;PC=U531%2fop.gg&amp;first=11&amp;FORM=PERE', '/search?q=bing&amp;cvid=077c2aeecd9e4fd0972d68f9cb5d36ac&amp;PC=U531%2fop.gg&amp;first=21&amp;FORM=PERE1', '/search?q=bing&amp;cvid=077c2aeecd9e4fd0972d68f9cb5d36ac&amp;PC=U531%2fop.gg&amp;first=31&amp;FORM=PERE2', '/search?q=bing&amp;cvid=077c2aeecd9e4fd0972d68f9cb5d36ac&amp;PC=U531%2fop.gg&amp;first=41&amp;FORM=PERE3', '/search?q=bing&amp;cvid=077c2aeecd9e4fd0972d68f9cb5d36ac&amp;PC=U531%2fop.gg&amp;first=11&amp;FORM=PORE']\n",
      "can only concatenate list (not \"str\") to list\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.request as ur\n",
    "import re\n",
    "import time\n",
    "r = ur.urlopen(\"https://cn.bing.com/search?q=bing&cvid=077c2aeecd9e4fd0972d68f9cb5d36ac&FORM=ANNTA1&PC=U531/op.gg\")\n",
    "content = r.read().decode('utf-8')\n",
    "href = re.compile(r'href=[\\'\"]?(/[^\\'\" >]+)')\n",
    "new_urls=href.findall(content)\n",
    "\n",
    "count = 0 # 层数\n",
    "floors = 1 # 限制爬取的层数\n",
    "lastStep = []\n",
    "r = re.compile(r'href=[\\'\"]?([^\\'\" >]+)')\n",
    "\n",
    "queue = [[new_urls]]\n",
    "for i in range(floors): # 限制爬取范围在设定的层数范围内\n",
    "    queue.append([])\n",
    "storage = {}\n",
    "used = set() # 设置集合存放爬取过的url\n",
    "while len(queue[0])>0 or count!=0: # 种子队列不为空或者层数不为零\n",
    "    try:\n",
    "        url = queue[count].pop(-1)\n",
    "        print(url+str(count)) # 打印当前链接和层数\n",
    "        html = ur.urlopen(url).read().decode('utf-8')\n",
    "        storage[url]=html\n",
    "        used.add(url) # 将爬取过的URL放入集合中\n",
    "        new_urls = r.findall(html)\n",
    "        if count < floors:\n",
    "            for new_url in set(new_urls):\n",
    "                if new_url not in used and new_url not in queue:#判断新链接网址中的包含的链接是否为重复的\n",
    "                    queue[count+1].append(new_url) # 将爬取的URL存入到队列中相应层数的列表\n",
    "            count+=1\n",
    "        else:\n",
    "            if len(queue[count])==0:\n",
    "                count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加上层数控制与去重之后，获取的数据明显减少，没有出现之前的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
