{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名，示例：``1_20188989899_张三``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http.client.HTTPResponse object at 0x000001E41D9F2C08>\n",
      "b'{\\n  \"args\": {}, \\n  \"headers\": {\\n    \"Accept-Encoding\": \"identity\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"Python-urllib/3.7\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-5e670ecc-5e44dc3643f8f472781f36a7\"\\n  }, \\n  \"origin\": \"58.45.16.45\", \\n  \"url\": \"http://httpbin.org/get\"\\n}\\n'\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python-urllib/3.7\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e670ecc-5e44dc3643f8f472781f36a7\"\n",
      "  }, \n",
      "  \"origin\": \"58.45.16.45\", \n",
      "  \"url\": \"http://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "url='http://httpbin.org/get'\n",
    "webPage=urllib.request.urlopen(url)\n",
    "print(webPage)\n",
    "data=webPage.read()\n",
    "print(data)\n",
    "print(data.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"key1\": [\n",
      "      \"value1\", \n",
      "      \"value2\"\n",
      "    ]\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"23\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.22.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e6764e5-ee7c407c65f159dfdf021b61\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"58.45.16.45\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这里编写代码\n",
    "\n",
    "import requests\n",
    "url = \"http://httpbin.org/post\"\n",
    "headers = {\n",
    "    \"User-Agent\":'Mozilla/4.0(compatible;MSIE 5.5;Windows NT)',\n",
    "    \"host\":'httpbin.org'\n",
    "}\n",
    "dict = {\n",
    "    \"name\":\"Hackdata\"\n",
    "}\n",
    "data = bytes(parse.urlencode(dict),encoding=\"utf8\")\n",
    "req = request.Request(url=url,data=data,headers=headers,method=\"POST\")\n",
    "response = request.urlopen(req)\n",
    "#print(response.read().decode(\"utf-8\"))\n",
    "payload_tuples = [('key1','value1'),('key1','value2')]\n",
    "r1 = requests.post('http://httpbin.org/post',data=payload_tuples)\n",
    "payload_dict = {'key1':['value1','value2']}\n",
    "r2 = requests.post('http://httpbin.org/post',data=payload_dict)\n",
    "print(r1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>\n",
      "BDORZ=27315\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"name\": \"Hackdata\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"13\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.22.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e6765f8-39cb94c07fae0120d0a31d20\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"58.45.16.45\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://www.baidu.com\")\n",
    "print(response.cookies)\n",
    "\n",
    "for key,value in response.cookies.items():\n",
    "    print(key+\"=\"+value)\n",
    "\n",
    "data = {\n",
    "    \"name\":\"Hackdata\"\n",
    "}\n",
    "response = requests.post(\"http://httpbin.org/post\",data=data)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,re\n",
    "count = 1.0\n",
    "r = re.compile(r'href=[\\''']?(http[^\\'''>]+)')\n",
    "seed = 'http://httpbin.org/'\n",
    "stack = [seed]\n",
    "storage = {}\n",
    "while len(queue)>0 and count>0:\n",
    "    try:\n",
    "        url = stack.pop(-1)\n",
    "        html = requests.get(url).text\n",
    "        new_urls = r.findall(html)\n",
    "        stack.extend(new_urls)\n",
    "        count-=1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,re\n",
    "count =1.0\n",
    "r=re.compile(r'href=[\\''']?(http[^\\'''>]+)')\n",
    "seed='http://httpbin.org/'\n",
    "queue=[seed]\n",
    "storage={}\n",
    "while len(queue)>0 and count>0:\n",
    "    try:\n",
    "        url = queue.pop(0)\n",
    "        html = requests.get(url).text\n",
    "        new_urls = r.findall(html)\n",
    "        queue.extend(new_urls)\n",
    "        count -=1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#广度优先遍历算法python实现\n",
    "def dfs(adj, start):\n",
    "    visited = set()\n",
    "    stack = [[start, 0]]\n",
    "    while stack:\n",
    "        (v, next_child_idx) = stack[-1]\n",
    "        if (v not in adj) or (next_child_idx >= len(adj[v])):\n",
    "            stack.pop()\n",
    "            continue\n",
    "        next_child = adj[v][next_child_idx]\n",
    "        stack[-1][1] += 1\n",
    "        if next_child in visited:\n",
    "            continue\n",
    "        print(next_child)\n",
    "        visited.add(next_child)\n",
    "        stack.append([next_child, 0])\n",
    " \n",
    " \n",
    "graph = {1: [4, 2], 2: [3, 4], 3: [4], 4: [5]}\n",
    "dfs(graph, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
