{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名+作业序号，示例：``1_20188989899_张三_1``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "200\n",
      "https://www.baidu.com/search/error.html\n",
      "Accept-Ranges: bytes\n",
      "Cache-Control: max-age=86400\n",
      "Content-Length: 15852\n",
      "Content-Type: text/html\n",
      "Date: Mon, 09 Mar 2020 15:17:11 GMT\n",
      "Etag: \"3dec-57b3a9a43af80\"\n",
      "Expires: Tue, 10 Mar 2020 15:17:11 GMT\n",
      "Last-Modified: Thu, 22 Nov 2018 06:01:50 GMT\n",
      "P3p: CP=\" OTI DSP COR IVA OUR IND COM \"\n",
      "Server: Apache\n",
      "Set-Cookie: BAIDUID=298EBCE53EB0CC9B934E8EDF3B99CC56:FG=1; expires=Tue, 09-Mar-21 15:17:11 GMT; max-age=31536000; path=/; domain=.baidu.com; version=1\n",
      "Vary: Accept-Encoding,User-Agent\n",
      "Connection: close\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request #发起一个不携带参数的get请求 \n",
    "\n",
    "response=urllib.request.urlopen('http://www.baidu.com/get')\n",
    "\n",
    "print(response.reason) #调用status属性可以此次请求响应的状态码，200表示此次请求成功\n",
    "\n",
    "print(response.status) #调用url属性，可以获取此次请求的地址\n",
    "\n",
    "print(response.url)\n",
    "\n",
    "print(response.headers) \n",
    "\n",
    "#由于使用read方法拿到的响应的数据是二进制数据，所有需要使用decode解码成utf-8编码\n",
    "# print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#发送get请求成功，并且得到了网站的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"{\\\"key1\\\": \\\"value1\\\", \\\"key2\\\": \\\"value2\\\"}\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"36\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.22.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e6663b8-c62b477569d804df601b6945\"\n",
      "  }, \n",
      "  \"json\": {\n",
      "    \"key1\": \"value1\", \n",
      "    \"key2\": \"value2\"\n",
      "  }, \n",
      "  \"origin\": \"58.44.7.146\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n",
      "b'{\\n  \"args\": {}, \\n  \"data\": \"{\\\\\"key1\\\\\": \\\\\"value1\\\\\", \\\\\"key2\\\\\": \\\\\"value2\\\\\"}\", \\n  \"files\": {}, \\n  \"form\": {}, \\n  \"headers\": {\\n    \"Accept\": \"*/*\", \\n    \"Accept-Encoding\": \"gzip, deflate\", \\n    \"Content-Length\": \"36\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"python-requests/2.22.0\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-5e6663b8-c62b477569d804df601b6945\"\\n  }, \\n  \"json\": {\\n    \"key1\": \"value1\", \\n    \"key2\": \"value2\"\\n  }, \\n  \"origin\": \"58.44.7.146\", \\n  \"url\": \"http://httpbin.org/post\"\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "import requests,json\n",
    "\n",
    "url_json='http://httpbin.org/post'\n",
    "\n",
    "data_json=json.dumps({'key1':'value1','key2':'value2'})\n",
    "\n",
    "#j将python对象解码为json数据\n",
    "\n",
    "r_json=requests.post(url_json,data_json)\n",
    "\n",
    "print(r_json)\n",
    "\n",
    "print(r_json.text)\n",
    "\n",
    "print(r_json.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#发送post请求成功，并且得到网站信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个session对象\n",
    "s = requests.Session()\n",
    "\n",
    "# 用session对象发出get请求，设置cookies\n",
    "s.get('http://baidu.com/cookies/set/sessioncookie/123456789')\n",
    "\n",
    "# 用session对象发出另外一个get请求，获取cookies\n",
    "r = s.get(\"http://baidu.com/cookies\")\n",
    "\n",
    "r.text\n",
    "'{\"cookies\":{\"sessioncookie\":\"123456789\"}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取cookies成功"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' # 这是一个神奇的网站，专门为爬虫练习而生\n",
    "queue = [seed]\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = stack.pop(-1)\n",
    "        html = requests.get(url).text\n",
    "        new_urls = r.findall(html)\n",
    "        stack.extend(new_urls)\n",
    "        storage[url] = len(new_urls)\n",
    "        count-=1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按深度遍历算法抓取网页（不去重）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://avatars0.githubusercontent.com\n",
      "HTTPSConnectionPool(host='avatars0.githubusercontent.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: SysCallError(10054, 'WSAECONNRESET')\")))\n",
      "https://avatars1.githubusercontent.com\n",
      "HTTPSConnectionPool(host='avatars1.githubusercontent.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: SysCallError(10054, 'WSAECONNRESET')\")))\n",
      "https://avatars2.githubusercontent.com\n",
      "HTTPSConnectionPool(host='avatars2.githubusercontent.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: SysCallError(10054, 'WSAECONNRESET')\")))\n",
      "https://avatars3.githubusercontent.com\n",
      "HTTPSConnectionPool(host='avatars3.githubusercontent.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: SysCallError(10054, 'WSAECONNRESET')\")))\n",
      "https://user-images.githubusercontent.com/\n",
      "HTTPSConnectionPool(host='user-images.githubusercontent.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: SysCallError(10054, 'WSAECONNRESET')\")))\n"
     ]
    }
   ],
   "source": [
    "import requests, re\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed ='http://httpbin.org/' \n",
    "# 这是一个神奇的网站，专门为爬虫练习而生 \n",
    "queue = [seed]\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url=queue.pop(0)\n",
    "        html=requests.get(url).text\n",
    "        new_urls=r.findall(html)\n",
    "        queue.extend(new_urls)\n",
    "        storage[url]=len(new_urls)\n",
    "        count-=1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按照广度优先算法抓取网页（不去重）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-11-328f355df49e>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-328f355df49e>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    print(e)\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import requests, re\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed ='http://httpbin.org/' \n",
    "# 这是一个神奇的网站，专门为爬虫练习而生 \n",
    "queue = [seed]\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url=queue.pop(0)\n",
    "        html=requests.get(url).text\n",
    "        new_urls=r.findall(html)\n",
    "        #将新发新未抓取的URL添加到queue中\n",
    "        for new_url in new_urls:\n",
    "            if new_url not in used and new_url not in queue:\n",
    "                queue.append(new_url)\n",
    "            count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按照广度优先算法抓取网页（去重）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
