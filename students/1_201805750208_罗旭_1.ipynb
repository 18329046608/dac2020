{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名+作业序号，示例：``1_20188989899_张三_1``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "# 定义基础网址\n",
    "base_url = \"http://httpbin.org/get\"\n",
    "# 构造字典参数\n",
    "data_dict = {\n",
    "    \"username\": \"201805750208\",\n",
    "    \"password\": \"123456789\"\n",
    "    # \"world\": \"hello\"\n",
    "}\n",
    "# 参数拼接到url地址后面\n",
    "data_string = urllib.parse.urlencode(data_dict,encoding=\"utf-8\")      # 使用urlencode这个方法将字典序列化成字符串\n",
    "print(data_string)\n",
    "new_url = base_url + \"?\" + data_string\n",
    "response = urllib.request.urlopen(new_url)\n",
    "print(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 传入的参数有\"username\",\"password\"\n",
    "##### 请求头中客户端支持自定义的编码类型;请求的服务器域名是 \"http://httpbin.org\" ; 跨域操作携带\"origin\";get方法直接将参数拼接到url地址后"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request, parse\n",
    "\n",
    "# 设置要访问的url\n",
    "url = \"http://httpbin.org/post\"\n",
    "# 设置字典参数\n",
    "dict = {\n",
    "    \"world\": \"hello\"\n",
    "}\n",
    "# 使用urlencode这个方法将字典序列化成字符串\n",
    "data = parse.urlencode(dict)\n",
    "# 自定义请求头\n",
    "headers = {\n",
    "    # 伪装成火狐浏览器\n",
    "    \"User-Agent\": \"Mozilla/5.5 (compatible; MSIE 5.5; Windows NT)\",\n",
    "    \"Host\": \"httpbin.org\"\n",
    "}\n",
    "# 将序列化后的字符串转换成二进制数据,以便post携带\n",
    "data = bytes(parse.urlencode(dict), encoding=\"utf-8\")\n",
    "req = request.Request(url=url, data=data, headers=headers, method=\"POST\")\n",
    "response = request.urlopen(req)\n",
    "print(response.read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 消息体长度为11；参数在\"form\"中; 客户端被改为火狐浏览器(\"User-Agent\": Mozilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cookies\": {\n",
      "    \"201805750208\": \"123456\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 创建session对象\n",
    "s = requests.Session()\n",
    "# 用此对象发送get请求，并设置cookies\n",
    "s.get(\"http://httpbin.org/cookies/set/201805750208/123456\")\n",
    "# 再次用session对象发出另一个get请求，获取cookies\n",
    "o = s.get(\"http://httpbin.org/cookies\")\n",
    "# 显示结果\n",
    "print(o.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 输出的cookies包含设置的内容\"201805750208:123456\"；存贮形式类似于字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/2019%E5%B9%B4%E6%AD%A6%E6%B1%89%E7%97%85%E6%AF%92%E6%80%A7%E8%82%BA%E7%82%8E%E7%97%85%E4%BE%8B/24236082', '/item/%E4%B8%96%E7%95%8C%E5%8D%AB%E7%94%9F%E7%BB%84%E7%BB%87/483426', '/item/%E4%B8%AD%E4%B8%9C%E5%91%BC%E5%90%B8%E7%BB%BC%E5%90%88%E5%BE%81/4894857', '/item/%E4%B8%A5%E9%87%8D%E6%80%A5%E6%80%A7%E5%91%BC%E5%90%B8%E7%BB%BC%E5%90%88%E5%BE%81/2942647', '/item/%E7%97%85%E6%AF%92%E6%80%A7%E8%82%BA%E7%82%8E/2251212', '/item/%E4%B8%96%E7%95%8C%E5%8D%AB%E7%94%9F%E7%BB%84%E7%BB%87/483426', '/item/2019%E5%B9%B4%E6%AD%A6%E6%B1%89%E7%97%85%E6%AF%92%E6%80%A7%E8%82%BA%E7%82%8E%E7%97%85%E4%BE%8B/24236082', '/item/2020%E5%B9%B4%E6%96%B0%E5%9E%8B%E5%86%A0%E7%8A%B6%E7%97%85%E6%AF%92%E7%96%AB%E6%83%85/24278151', '/item/%E6%9F%B3%E5%8F%B6%E5%88%80/7955065', '/item/%E6%AD%A6%E6%B1%89%E5%8A%A0%E6%B2%B9/24289649', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n",
      "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E8%BA%AB%E4%BB%BD%E8%AF%81/113951', '/item/%E6%88%B7%E5%8F%A3%E6%9C%AC/5104621', '/item/%E4%B9%89%E9%A1%B9/6176882', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n",
      "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E8%BA%AB%E4%BB%BD%E8%AF%81/113951', '/item/%E6%88%B7%E5%8F%A3%E6%9C%AC/5104621', '/item/%E4%B9%89%E9%A1%B9/6176882', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n",
      "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E8%BA%AB%E4%BB%BD%E8%AF%81/113951', '/item/%E6%88%B7%E5%8F%A3%E6%9C%AC/5104621', '/item/%E4%B9%89%E9%A1%B9/6176882', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n",
      "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E8%BA%AB%E4%BB%BD%E8%AF%81/113951', '/item/%E6%88%B7%E5%8F%A3%E6%9C%AC/5104621', '/item/%E4%B9%89%E9%A1%B9/6176882', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n"
     ]
    }
   ],
   "source": [
    "from urllib import request as ur\n",
    "import re\n",
    "\n",
    "count = 0  # 层数\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)') # 抽取所需链接信息的正则语言规则\n",
    "# 种子为\"新型冠状病毒\"词条\n",
    "seed = \"/item/2019%E6%96%B0%E5%9E%8B%E5%86%A0%E7%8A%B6%E7%97%85%E6%AF%92?fromtitle=%E6%96%B0%E5%9E%8B%E5%86%A0%E7%8A%B6%E7%97%85%E6%AF%92&fromid=7904360\"\n",
    "stack = [seed]  # 用栈实现深度优先算法\n",
    "storage = {}\n",
    "while count < 5:\n",
    "    try:\n",
    "        url = stack.pop(-1)  # 取出栈的最后一条URL\n",
    "        html = ur.urlopen(\"https://baike.baidu.com\"+url).read().decode(\"utf-8\") # 拼接URL\n",
    "        new_urls = r.findall(html)  # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)\n",
    "        stack.extend(new_urls)      # 新提取的URL入栈\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1                  # 迭代次数加1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 迭代五次，最终有五个列表；每个列表当中的部分加上 \"http://baike.baidu\" 均可访问相应界面; 由于网页最后一个链接是相同的，又因为深度遍历特性，所以后四个完全相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不去重的广度优先爬取\n",
    "from urllib import request as ur\n",
    "import re\n",
    "\n",
    "count = 0   # 层数\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)')  # 抽取所需链接信息的正则语言规则\n",
    "# 种子为\"新型冠状病毒\"词条\n",
    "seed = \"/item/%E7%BE%8E%E5%9B%A2%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1?fromtitle=%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1&fromid=17501927\"\n",
    "queue = [seed]   # 用队列实现广度优先算法\n",
    "storage = {}\n",
    "while count < 5:\n",
    "    try:\n",
    "        url = queue.pop(0)   # 取出队列第一条URL\n",
    "        html = ur.urlopen(\"https://baike.baidu.com\"+url).read().decode('utf-8')  # 拼接URL\n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)\n",
    "        queue.extend(new_urls) # 将新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request as ur\n",
    "import re\n",
    "\n",
    "#加层数控制的\n",
    "count = 0 # 层数\n",
    "floors = 2 # 限制爬取的层数\n",
    "lastStep = []\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)')\n",
    "seed = '/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB' # 这是网络爬虫词条\n",
    "queue = [[seed]]\n",
    "for i in range(floors): # 限制爬取范围在设定的层数范围内\n",
    "    queue.append([])\n",
    "storage = {}\n",
    "used = set() # 设置集合存放爬取过的url\n",
    "while len(queue[0])>0 or count!=0: # 种子队列不为空或者层数不为零\n",
    "    try:\n",
    "        url = queue[count].pop(-1)\n",
    "        print(url+\" \"+str(count)) # 打印当前链接和层数\n",
    "        html = ur.urlopen('https://baike.baidu.com'+url).read().decode('utf-8')\n",
    "        storage[url] = html\n",
    "        used.add(url) # 将爬取过的URL放入集合中\n",
    "        new_urls = r.findall(html)\n",
    "        if count < floors:\n",
    "            for new_url in set(new_urls):\n",
    "                if new_url not in used and new_url not in queue: # 判断新链接网址中的包含的链接是否为重复的\n",
    "                    queue[count+1].append(new_url)  # 将爬取的URL存入到队列中相应层数的列表\n",
    "            if len(queue[count]) == 0:             #  这行代码确保遍历方式为广度优先\n",
    "                count += 1\n",
    "        else:\n",
    "            if len(queue[count])==0:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 去重和不去重在某些情况下有相当大的区别；可以通过设置爬取的层数作为爬取策略的一部分;部分分析已经放在代码解释中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reptile():\n",
    "    def __init__(self, count, floors, seed):\n",
    "        self.count = count\n",
    "        self.floors = floors\n",
    "        self.seed = seed\n",
    "        self.action()\n",
    "\n",
    "    def action(self):\n",
    "        stack = [[self.seed]]\n",
    "        for i in range(self.floors):  # 限制爬取范围在设定的层数范围内\n",
    "            stack.append([])\n",
    "        storage = {}\n",
    "        r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)')\n",
    "        used = set()  # 设置集合存放爬取过的url\n",
    "        while len(stack[0]) > 0 or self.count != 0:  # 种子队列不为空或者层数不为零\n",
    "            try:\n",
    "                url = stack[self.count].pop(-1)\n",
    "                print(url + \" \" + str(self.count))  # 打印当前链接和层数\n",
    "                html = ur.urlopen('https://baike.baidu.com' + url).read().decode('utf-8')\n",
    "                storage[url] = html\n",
    "                used.add(url)  # 将爬取过的URL放入集合中\n",
    "                new_urls = r.findall(html)\n",
    "                if self.count < self.floors:\n",
    "                    for new_url in set(new_urls):\n",
    "                        if new_url not in used and new_url not in stack:  # 判断新链接网址中的包含的链接是否为重复的\n",
    "                            stack[self.count + 1].append(new_url)  # 将爬取的URL存入到队列中相应层数的列表\n",
    "                    self.count += 1\n",
    "                else:\n",
    "                    if len(stack[self.count]) == 0:\n",
    "                        self.count -= 1\n",
    "            except Exception as e:\n",
    "                print(url)\n",
    "                print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request as ur\n",
    "import re\n",
    "count = 0  # 层数\n",
    "floors = 2  # 限制爬取的层数\n",
    "seed = '/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB'  # 这是网络爬虫词条\n",
    "a = Reptile(count, floors, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
