{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名+作业序号，示例：``1_20188989899_张三_1``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"args\": {}, \\n  \"data\": \"\", \\n  \"files\": {}, \\n  \"form\": {\\n    \"word\": \"hello\"\\n  }, \\n  \"headers\": {\\n    \"Accept-Encoding\": \"identity\", \\n    \"Content-Length\": \"10\", \\n    \"Content-Type\": \"application/x-www-form-urlencoded\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"Python-urllib/3.7\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-5e6636a6-3c574db3c6be4677cf3a0d19\"\\n  }, \\n  \"json\": null, \\n  \"origin\": \"58.44.244.108\", \\n  \"url\": \"http://httpbin.org/post\"\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "# 这里编写代码\n",
    "import urllib.request\n",
    "data = bytes(urllib.parse.urlencode({'word': 'hello'}),encoding='utf8')              # 相当于一个将{'word': 'hello'}转变为计算机可识别语言的步骤\n",
    "response = urllib.request.urlopen('http://httpbin.org/post',data=data)               # 用urlopen发送请求\n",
    "print(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里对结果进行分析。\n",
    "\n",
    "\n",
    "本来我测试的是网址https://www.cnblogs.com/vamei/archive/2012/05/29/2524376.html\n",
    "但是得出来的反馈太长了，里面还没有我认识的，所以就换成了ppt上的例子\n",
    "\n",
    "\n",
    "+ headers            头\n",
    "+ Accept-Encoding       可用的压缩方式      identity\n",
    "+ Content-Length        消息体长度          10\n",
    "+ Content-Type         消息体类型         application/x-www-form-urlencoded\n",
    "+ host              主机             httpbin.org\n",
    "+ User-Agent          客户端身份          Python-urllib/3.7\n",
    "+ X-Amzn-Trace-Id      唯一识别代码        Root=1-5e6631a7-c53d164851146c308058bb20\n",
    "+ url              统一地址           http://httpbin.org/post\"\\n}\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"name\": \"Hackdata\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"13\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e6636aa-f6b1c998e34b7860c489f018\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"58.44.244.108\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这里编写代码\n",
    "from urllib import request,parse\n",
    "\n",
    "url = \"http://httpbin.org/post\"\n",
    "headers = {\"User-Agent\":'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',\"host\":'httpbin.org'}# 用火狐浏览器进行伪装（客户端身份）\n",
    "dict = {\"name\":\"Hackdata\"}                                                                    # form改变\n",
    "data = bytes(parse.urlencode(dict),encoding=\"utf8\")\n",
    "req = request.Request(url=url,data=data,headers=headers,method=\"POST\")                        # 用Request发送定制请求\n",
    "response = request.urlopen(req)\n",
    "print(response.read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里对结果进行分析。\n",
    "\n",
    "与上面的大致变化如下：\n",
    "\n",
    "1. 最后输出print(response.read().decode(\"utf-8\"))使得反馈信息\\n直接转移成了换行符。\n",
    "2. form从字典{'word': 'hello'}变成了字典{\"name\": \"Hackdata\"}\n",
    "3. 消息体长度从10变成了13\n",
    "4. 因为进行了火狐浏览器伪装，所以客户端身份User-Agent的python3.7发生了改变\n",
    "5. 唯一标识发生了改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"cookies\": {\\n    \"sessioncookie\": \"123456789\"\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里编写代码\n",
    "import requests\n",
    "\n",
    "s = requests.Session()                                                    # 创建一个session对象 \n",
    "s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')           # 用session对象发出get请求，设置cookies\n",
    "r = s.get(\"http://httpbin.org/cookies\")                                   # 用session对象发出另外一个get请求，获取cookies\n",
    "r.text                                                                    # 显示结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里对结果进行分析。\n",
    "对步骤的分析已经放在了代码注释中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "\n",
    "import requests, re\n",
    "\n",
    "count = 3                                                                      # 爬取次数（层数）\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://cookdata.cn/course/course_introduction/1/'                      # 选择要爬取的种子网站\n",
    "queue = [seed]                                                                 # 设置一个队列，用以存储接下来要爬取的网站的地址\n",
    "used = set()                                                                   # 设置一个集合，保存已经抓取过的 URL\n",
    "storage = {}                                      \n",
    "\n",
    "while len(queue) > 0 and count > 0:                                            # 队列不空且爬取次数不为零（按要求还要继续爬时）\n",
    "    try:\n",
    "        url = queue.pop(0)                                                     # 得到队列中的第一个网站\n",
    "        html = requests.get(url).text                                          # 发送get请求，并得到消息体\n",
    "        storage[url] = html                    \n",
    "        used.add(url)                                                          # 将已经抓取过的URL存入used集合中\n",
    "        new_urls = r.findall(html)                                             # re的方法，将该网站上所有的有关于http开头的url找到并作为列表存储\n",
    "        for new_url in new_urls:                                               # 此循环将未抓取的URL添加到queue中\n",
    "            if new_url not in queue:                                           # 没有去重，去重的代码在这里if里面加XXXXX not in used and XXXXXX\n",
    "                queue.append(new_url)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)\n",
    "        count -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里对结果进行分析。\n",
    "对步骤的分析已经放在了代码的注释中\n",
    "\n",
    "Failed to establish a new connection 无法建立新连接 Errno 11002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "# 这里写的是没有去重的遍历算法，去重的只要在if条件语句里面稍微变化一下就行了，略\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import requests, re\n",
    "\n",
    "init_url = \"https://cn.hao123.com/\"\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "queue = [init_url]\n",
    "used = []\n",
    "maxnum = 5\n",
    "\n",
    "while len(queue) > 0 and maxnum > 0:\n",
    "    try:\n",
    "        url = queue.pop(0)\n",
    "        html = requests.get(url).text\n",
    "        used.append(url)                                                       # 将已经抓取过的URL存入used集合中\n",
    "        new_urls = r.findall(html)                                             # 找到接下来要爬的网站\n",
    "        for new_url in new_urls:\n",
    "            if new_url not in queue:\n",
    "                queue.append(new_url)\n",
    "        maxnum -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)\n",
    "        maxnum -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里对结果进行分析。\n",
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "import requests, re\n",
    "\n",
    "class Crawler_URL:\n",
    "    \n",
    "    def __init__(self,url):                             # 相当于一个初始化的操作\n",
    "        self.url = url\n",
    "        \n",
    "    def Get_All_URL(self,count):                        # 定义类下面的一个方法，用以爬取种子网站的相关网站\n",
    "        r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "        queue = [self.url]\n",
    "        used = set()\n",
    "        \n",
    "        while len(queue) > 0 and maxnum > 0:\n",
    "            try:\n",
    "                url = queue.pop(0)\n",
    "                html = requests.get(url).text\n",
    "                used.add(url)\n",
    "                new_urls = r.findall(html)\n",
    "                for new_url in new_urls:\n",
    "                    if new_url not in used and new_url not in queue:\n",
    "                        queue.append(new_url)\n",
    "                maxnum -= 1\n",
    "            except Exception as e:\n",
    "                print(url)\n",
    "                print(e)\n",
    "                maxnum -= 1\n",
    "        return list(used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Crawler_URL.Get_All_URL of <__main__.Crawler_URL object at 0x000002361D281F60>>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "url = Crawler_URL(\"http://cookdata.cn/course/course_introduction/1/\")\n",
    "url.Get_All_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个Crawler_URL的类，用以创建Crawler_URL的对象。\n",
    "Crawler_URL的对象下有Get_All_URL的方法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
