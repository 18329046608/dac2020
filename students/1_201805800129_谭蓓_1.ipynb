{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名，示例：``1_20188989899_张三``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python-urllib/3.7\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e64c183-08335714004649e0c687e450\"\n",
      "  }, \n",
      "  \"origin\": \"117.136.88.169\", \n",
      "  \"url\": \"http://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "url='http://httpbin.org/get'\n",
    "# 打开url,返回一个http对象，请求方式为get\n",
    "response = request.urlopen(url)\n",
    "# 输出文本信息，对获得的网页内容解码\n",
    "print(response.read().decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "headers：请求头；host：请求的主机名称；Accept-Encoding：可用的压缩方式；User-Agent：客户端的身份"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"name\": \"python\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"11\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e64c5d6-289b13b8b8c98f66cda88912\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"117.136.88.169\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url='http://httpbin.org/post'\n",
    "#伪装一个火狐浏览器\n",
    "headers= {\n",
    "    \"User-Agent\":'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',\n",
    "    \"host\":'httpbin.org'\n",
    "         }\n",
    "data={'name':'python'}\n",
    "#用requests库发送post请求\n",
    "response=requests.post(url=url,data=data,headers=headers) \n",
    "#显示结果\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-Length：消息体的长度；Content-Type：消息体的类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cookies\": {\n",
      "    \"sessioncookie\": \"66666666\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# 创建一个session对象 \n",
    "S = requests.Session() \n",
    "# 发出get请求，设置cookies \n",
    "S.get('http://httpbin.org/cookies/set/sessioncookie/66666666') \n",
    "# 用session对象发出另外一个get请求，获取cookies \n",
    "re = S.get(\"http://httpbin.org/cookies\") \n",
    "# 显示结果 \n",
    "print(re.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以字典的形式返回cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/\n",
      "http://jianyi.baidu.com/\n",
      "http://rj.baidu.com/soft/detail/11843.html?ald\n",
      "http://www.baidu.com/link?url=01vNBVXR2eaJxETl9PI3hcrvKCcwaJIKk5FkpO7l5YI_Q_pC24ogIBoZxI0LAu5oYpAdhRH42nzxAqfui0YnHK\n",
      "http://index.baidu.com\n"
     ]
    }
   ],
   "source": [
    "# 不去重的深度遍历\n",
    "import requests, re\n",
    "count = 5  #控制循环的次数\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://www.baidu.com/'  # 种子网站\n",
    "stack=[seed] #创建一个list对象\n",
    "storge={} # 创建一个字典对象，用于存储获取的文本内容\n",
    "while len(stack) > 0 and count > 0:\n",
    "        url = stack.pop(-1)  #弹出列表最后一个元素\n",
    "        html = requests.get(url).text  #发送get请求，获取文本内容\n",
    "        storge[url]=html\n",
    "        new_urls = r.findall(html)  #获取html中所有与r相匹配的子串，并以列表的形式返回\n",
    "        stack.extend(new_urls)      # 将新发现的URL添加到stack中\n",
    "        count -= 1\n",
    "        print(url)  #打印url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述为在5次不去重的深度遍历下得到的url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/\n",
      "http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css\n",
      "http://news.baidu.com\n",
      "http://www.hao123.com\n",
      "http://map.baidu.com\n",
      "http://v.baidu.com\n",
      "http://tieba.baidu.com\n",
      "http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1\n",
      "http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\n",
      "http://home.baidu.com\n",
      "http://ir.baidu.com\n",
      "http://www.baidu.com/duty/\n",
      "http://jianyi.baidu.com/\n",
      "https://redirect.simba.taobao.com/rd?c=un&amp;w=bd&amp;f=https%3A%2F%2Fs.click.taobao.com%2Ft%3Fe%3Dm%253D2%2526s%253DxJCnmwKDbSgcQipKwQzePCperVdZeJviK7Vc7tFgwiFRAdhuF14FMTIMalfKZWT8xq3IhSJN6GT%252FTrTNBNETjAtOHPHN0vssKO4N%252F%252F7xLcXH%252B5ZHu43PYJHjKlMEkHC0LxRPKN2FDAck%252FCKKDVvEs8hC7k91emiMAFeBK6%252FtK%252Fsq2x4KkidPL%252BReR15rySBjKYFnllaKQ3uiZ%252BQMlGz6FQ%253D%253D&amp;k=67a22f436b17a341&amp;p=mm_26632322_6858406_70736499\n",
      "https://pan.baidu.com/\n"
     ]
    }
   ],
   "source": [
    "# 不去重\n",
    "import requests, re\n",
    "count = 15  #控制循环的次数\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://www.baidu.com/'  # 种子网站\n",
    "queue=[seed]\n",
    "storge={} # 创建一个字典对象，用于存储获取的文本内容\n",
    "while len(queue) > 0 and count > 0:\n",
    "        url = queue.pop(0)  #弹出队首元素\n",
    "        html = requests.get(url).text  #发送get请求，获取文本内容\n",
    "        storge[url]=html\n",
    "        new_urls = r.findall(html)  #获取html中所有与r相匹配的子串，并以列表的形式返回\n",
    "        queue.extend(new_urls)      # 将新发现的URL添加到queue中\n",
    "        count -= 1\n",
    "        print(url)  #打印url\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述为在15次不去重的情况下获取的url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/\n",
      "http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css\n",
      "http://news.baidu.com\n",
      "http://www.hao123.com\n",
      "http://map.baidu.com\n",
      "http://v.baidu.com\n",
      "http://tieba.baidu.com\n",
      "http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1\n",
      "http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\n",
      "http://home.baidu.com\n",
      "http://ir.baidu.com\n",
      "http://www.baidu.com/duty/\n",
      "http://jianyi.baidu.com/\n",
      "https://www.baidu.com/\n",
      "http://tieba.baidu.com/\n"
     ]
    }
   ],
   "source": [
    "# 去重\n",
    "import requests, re\n",
    "count = 15 #控制循环的次数\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://www.baidu.com/' # 种子网站\n",
    "queue = [seed]\n",
    "used = set() # 设置一个集合，保存已经抓取过的 URL\n",
    "while len(queue) > 0 and count > 0:\n",
    "        url = queue.pop(0) #弹出队首元素\n",
    "        html = requests.get(url).text #发送get请求，获取文本内容\n",
    "        used.add(url) #将已经抓取过的URL放入used集合中\n",
    "        new_urls = r.findall(html) #获取html中所有与r相匹配的子串，并以列表的形式返回\n",
    "        # 将新发现的新未抓取的URL添加到queue中\n",
    "        for new_url in new_urls:\n",
    "            if new_url not in used or new_url not in queue:\n",
    "                queue.append(new_url)  \n",
    "        count -= 1\n",
    "        print(url) #打印URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述为在15次去重的情况下获取的url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
