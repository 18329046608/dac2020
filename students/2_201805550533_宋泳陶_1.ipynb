{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名，示例：``1_20188989899_张三``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "类： <class 'requests.models.Response'> \n",
      "\n",
      "头部信息： {'Date': 'Tue, 10 Mar 2020 05:11:01 GMT', 'Content-Type': 'application/json', 'Content-Length': '421', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true'} \n",
      "\n",
      "HTTP响应内容的字符串形式： {\n",
      "  \"args\": {\n",
      "    \"key1\": \"value1\", \n",
      "    \"key2\": [\n",
      "      \"value2\", \n",
      "      \"value3\"\n",
      "    ]\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.19.1\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e672165-7669ba46d3368c08b61ad37c\"\n",
      "  }, \n",
      "  \"origin\": \"223.100.71.215\", \n",
      "  \"url\": \"http://httpbin.org/get?key1=value1&key2=value2&key2=value3\"\n",
      "}\n",
      "\n",
      "HTTP响应内容的二进制形式：\n",
      " b'{\\n  \"args\": {\\n    \"key1\": \"value1\", \\n    \"key2\": [\\n      \"value2\", \\n      \"value3\"\\n    ]\\n  }, \\n  \"headers\": {\\n    \"Accept\": \"*/*\", \\n    \"Accept-Encoding\": \"gzip, deflate\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"python-requests/2.19.1\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-5e672165-7669ba46d3368c08b61ad37c\"\\n  }, \\n  \"origin\": \"223.100.71.215\", \\n  \"url\": \"http://httpbin.org/get?key1=value1&key2=value2&key2=value3\"\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "# 这里编写代码\n",
    "import requests                                              #调用requests库\n",
    "payload = {'key1': 'value1', 'key2': ['value2', 'value3']}\n",
    "r = requests.get('http://httpbin.org/get', params=payload)   #获取HTML网页信息 \n",
    "print(r.status_code)                                         #查看状态\n",
    "print('类：',type(r),'\\n')                                   #查看response类\n",
    "print('头部信息：',r.headers,'\\n')                           #获取网页头部信息\n",
    "print('HTTP响应内容的字符串形式：',r.text)                   #获取HTTP相应内容（字符串形式）\n",
    "print('HTTP响应内容的二进制形式：\\n',r.content)              #获取HTTP相应内容（二进制形式）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* r.status_code :200为网页请求成功;404网页找不到\n",
    "* type(r) :<class 'requests.models.Response'> \n",
    "* r.headers:头部信息；Date、Content-Type、Content-Length、Connection、Server、Access-Control-Allow-Origin、Access-Control-Allow-Credentials...\n",
    "* r.text:网页内容：head、body\n",
    "#### get用法\n",
    "~~~\n",
    "payload = {'key1': 'value1', 'key2': ['value2', 'value3']}\n",
    "r = requests.get('http://httpbin.org/get', params=payload)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"name\": \"Hackdata\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\", \n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\", \n",
      "    \"Content-Length\": \"13\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e672166-5e947e511c03a7fc064c5c2c\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"223.100.71.215\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这里编写代码\n",
    "from urllib import request,parse\n",
    "                  \n",
    "url='http://httpbin.org/post'    #网址\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\"\n",
    "    }  #伪装成火狐浏览器\n",
    "dict = {\n",
    "    \"name\":\"Hackdata\"\n",
    "}\n",
    "data = bytes(parse.urlencode(dict),encoding=\"utf8\")  #编码形式为'utf-8'\n",
    "req = request.Request(url=url,data=data,headers=headers,method=\"POST\") #Request（）的一个结构\n",
    "response = request.urlopen(req)\n",
    "print(response.read().decode(\"utf-8\"))  #输出response文本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在火狐浏览器页面上调试源码，刷新，在network;headers中调取浏览器相关信息；\n",
    "* requests.post：伪装火狐浏览器发起post请求\n",
    "* response.text：输出网页文本\n",
    "#### post用法\n",
    "~~~\n",
    "payload_tuples = [('key1', 'value1'), ('key1', 'value2')]\n",
    "r1 = requests.post('http://httpbin.org/post', data=payload_tuples)\n",
    "payload_dict = {'key1': ['value1', 'value2']}\n",
    "r2 = requests.post('http://httpbin.org/post', data=payload_dict)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"cookies\": {\\n    \"sessioncookie\": \"123456789\"\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里编写代码\n",
    "import requests                           #调用requests库 \n",
    "s=requests.Session()                      #创建一个session对象 \n",
    "s.get('http://httpbin.org/cookies/set/sessioncookie/123456789') #用session对象发出get请求，设置cookies\n",
    "r=s.get(\"http://httpbin.org/cookies\")     #用session对象发出另外一个get请求，获取cookies\n",
    "r.text                                    # 显示结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~ \n",
    "'{ \"cookies\": { \"sessioncookie\": \"123456789\"}}' \n",
    "~~~\n",
    "* 不用反复发送cookie维持链接，响应速度提升明显。\n",
    "* 现在网站保持登录状态与验证都是使用session。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "def depth_tree(tree_node):\n",
    "    if tree_node is not None:\n",
    "        print(tree_node._data)\n",
    "        if tree_node._left is not None:\n",
    "            return depth_tree(tree_node._left)\n",
    "        if tree_node._right is not None:\n",
    "            return depth_tree(tree_node._right)\n",
    "#list模拟栈，实现DFS算法    深度优先\n",
    "import requests, re\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' # 爬虫网站\n",
    "queue = [seed]\n",
    "stack = [seed]\n",
    "storage = {}\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = stack.pop(-1)\n",
    "        html = requests.get(url,verify=False).text\n",
    "        new_urls = r.findall(html)\n",
    "        stack.extend(new_urls)\n",
    "        storage[url] = len(new_urls)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 深度优先算法\n",
    "~~~\n",
    "def depth_tree(tree_node):\n",
    "    if tree_node is not None:\n",
    "        print(tree_node._data)\n",
    "        if tree_node._left is not None:\n",
    "            return depth_tree(tree_node._left)\n",
    "        if tree_node._right is not None:\n",
    "            return depth_tree(tree_node._right)\n",
    "~~~\n",
    "* 深度优先从根节点开始，沿着一条路径尽可能深地访问，直到遇到叶节点时才回溯。\n",
    "* 深度优先有自己的优点，更容易陷入无限循环。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码 广度优先遍历\n",
    "def level_queue(root):\n",
    "    if root is None:\n",
    "        return\n",
    "    my_queue=[]\n",
    "    node=root\n",
    "    my_quere.append(node)\n",
    "    while my_quere:\n",
    "        node = my_quere.pop(0)\n",
    "        print(node.elem)\n",
    "        if node.lchild is not None:\n",
    "            my_quere.append(node.lchild)\n",
    "        if node.rchild is not None:\n",
    "            my_quere.append(node.rchild)\n",
    "            \n",
    "#list模拟队列，实现BFS算法（不去重）广度优先\n",
    "import requests, re\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' # 这是一个神奇的网站，专门为爬虫练习而生\n",
    "queue = [seed]\n",
    "storage = {}\n",
    "requests.packages.urllib3.disable_warnings()  #防止警告\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = queue.pop(0)\n",
    "        html = requests.get(url,verify=False).text\n",
    "        new_urls = r.findall(html)\n",
    "        queue.extend(new_urls)\n",
    "        storage[url] = len(new_urls)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list模拟队列，实现BFS算法（Hash去重）广度优先\n",
    "storage = {}\n",
    "import requests, re\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' # 这是一个神奇的网站，专门为爬虫练习而生\n",
    "queue = [seed]\n",
    "used = set() # 设置一个集合，保存已经抓取过的 URL\n",
    "storage = {}\n",
    "requests.packages.urllib3.disable_warnings()  #防止警告\n",
    "while len(queue) > 0 and count > 0: \n",
    "    try:\n",
    "        url = queue.pop(0)\n",
    "        html = requests.get(url,verify=False).text\n",
    "        storage[url] = html\n",
    "        #将已经抓取过的URL存入used集合中\n",
    "        used.add(url)\n",
    "        new_urls = r.findall(html)\n",
    "        #将新发新未抓取的URL添加到queue中\n",
    "        for new_url in new_urls:\n",
    "            if new_url not in used and new_url not in queue:\n",
    "                queue.append(new_url)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 使用广度优先策略的原因：\n",
    "~~~\n",
    "1.重要的网页往往离种子站点距离较近\n",
    "2.互联网的深度没有那么深，但却出乎意料地宽广。\n",
    "~~~\n",
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "#list模拟栈，实现DFS算法（Hash去重）深度优先\n",
    "import requests, re  #设置一个Python的数据类型-集合，来保存已经爬取过的URL\n",
    "count = 3\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' # 这是一个神奇的网站，专门为爬虫练习而生\n",
    "queue = [seed]\n",
    "used = set() # 设置一个集合，保存已经抓取过的 URL\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0: #Hash去重\n",
    "    try:\n",
    "        url = queue.pop(0)\n",
    "        html = requests.get(url).text\n",
    "        storage[url] = html\n",
    "        #将已经抓取过的URL存入used集合中\n",
    "        used.add(url)\n",
    "        new_urls = r.findall(html)\n",
    "        #将新发新未抓取的URL添加到queue中\n",
    "        for new_url in new_urls:\n",
    "            if new_url not in used and new_url not in queue:\n",
    "                queue.append(new_url)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://kennethreitz.org 1\n",
      "https://github.com/rochacbruno/flasgger 1\n",
      "https://github.githubassets.com 1\n",
      "https://avatars0.githubusercontent.com 1\n",
      "https://avatars1.githubusercontent.com 1\n",
      "https://avatars2.githubusercontent.com 1\n",
      "https://avatars3.githubusercontent.com 1\n",
      "https://github-cloud.s3.amazonaws.com 1\n",
      "https://user-images.githubusercontent.com/ 1\n",
      "https://github.githubassets.com/assets/frameworks-faf8983e496d05877e58c5b73e184080.css 1\n"
     ]
    }
   ],
   "source": [
    "# 测试统计URL管理器中各URL的数量\n",
    "from collections import Counter\n",
    "url_count = Counter(queue)\n",
    "for url,count in url_count.most_common(10):\n",
    "    print(url,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
