{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\r\n",
      "<head>\r\n",
      "\t<script>\r\n",
      "\t\tlocation.replace(location.href.replace(\"https://\",\"http://\"));\r\n",
      "\t</script>\r\n",
      "</head>\r\n",
      "<body>\r\n",
      "\t<noscript><meta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"></noscript>\r\n",
      "</body>\r\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "response = urllib.request.urlopen(\"https://www.baidu.com\")\n",
    "print(response.read().decode(\"utf-8\"))#文本输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用``urllib.requests``库发起了一次``get``请求，输出一些文本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"name\": \"Hackdata\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"13\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/4.0(compatible;MSIE 5.5;Windows NT)\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e664696-ff758836b00fc51600f846f4\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"58.209.76.51\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import request,parse\n",
    "\n",
    "url=\"http://httpbin.org/post\"\n",
    "headers = {\n",
    "    \"User-Agent\":'Mozilla/4.0(compatible;MSIE 5.5;Windows NT)',#伪装一个火狐浏览器\n",
    "    \"host\":'httpbin.org'}\n",
    "dict = {\n",
    "    \"name\":\"Hackdata\"}\n",
    "data = bytes(parse.urlencode(dict),encoding=\"utf8\")\n",
    "req=request.Request(url=url,data=data,headers=headers,method=\"POST\")#发起一次post请求\n",
    "response=request.urlopen(req)\n",
    "print(response.read().decode(\"utf-8\"))#输出response文本信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出了文本信息，包括标题、起源、网址等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"cookies\": {\\n    \"sessioncookie\": \"123456789\"\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "s=requests.session()# 创建一个session对象 \n",
    "s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')# 发出get请求，设置cookies \n",
    "r = s.get(\"http://httpbin.org/cookies\")#获取cookies \n",
    "r.text#输出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，然后输出了设置的``cookies``内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://fonts.googleapis.com/css?family=Open+Sans:400,700|Source+Code+Pro:300,600|Titillium+Web:400,600,700', 'https://github.com/requests/httpbin', 'https://kennethreitz.org', 'https://github.com/rochacbruno/flasgger']\n"
     ]
    }
   ],
   "source": [
    "import requests, re\n",
    "count =1  #设置深度\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' # 这是一个神奇的网站，专门为爬虫练习而生\n",
    "stack = [seed]    #使用list模拟栈\n",
    "storage = {}   \n",
    "while len(stack)>0 and count > 0:\n",
    "    try:\n",
    "        url = stack.pop(-1)\n",
    "        html = requests.get(url).text   #发送get请求\n",
    "        new_urls = r.findall(html)\n",
    "        stack.extend(new_urls)\n",
    "        storage[url] = len(new_urls)\n",
    "        count -= 1      #记数控制\n",
    "        print(stack)\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用了深度遍历算法，没有去重，实现了抓取网页并输出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去重\n",
    "import requests, re\n",
    "count = 3\n",
    "recount=0\n",
    "allcount=1\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' \n",
    "queue = [seed]\n",
    "used = set() # 设置一个集合，保存已经抓取过的 URL\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = queue.pop(0)\n",
    "        html = requests.get(url).text\n",
    "        storage[url] = html#将已经抓取过的URL存入used集合中\n",
    "        used.add(url)\n",
    "        new_urls = r.findall(html)#将新发新未抓取的URL添加到queue中\n",
    "        for new_url in new_urls:\n",
    "            allcount+=1\n",
    "            if new_url not in used and new_url not in queue:\n",
    "                queue.append(new_url)\n",
    "            else:\n",
    "                print(\"重复：\"+new_url)\n",
    "                recount += 1\n",
    "        count-=1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#不去重\n",
    "import requests, re\n",
    "count = 3\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' # 这是一个神奇的网站，专门为爬虫练习而生\n",
    "queue = [seed]\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0:    #用len方法判断是否为空\n",
    "    try:\n",
    "        url = queue.pop(0)    #使用list模拟队列\n",
    "        html = requests.get(url).text\n",
    "        new_urls = r.findall(html)\n",
    "        queue.extend(new_urls)    #新的url都添加到末尾\n",
    "        storage[url] = len(new_urls)\n",
    "        count -= 1\n",
    "        print(queue)\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用广度遍历算法,实现了抓取网页并输出结果,经过去重操作后，重复的网址都被标注了出来"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
