{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名+作业序号，示例：``1_20188989899_张三_1``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.22.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e6513c2-243185be0f747f6d57e39788\"\n",
      "  }, \n",
      "  \"origin\": \"211.97.123.227\", \n",
      "  \"url\": \"http://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://httpbin.org/get\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "响应返回了响应头headers等信息、域名及url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"age\": \"20\", \n",
      "    \"name\": \"Jasson\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"18\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.22.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e651ba2-99358b76fe44e3bcceec7233\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"211.97.123.227\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"name\" : \"Jasson\",\n",
    "    \"age\" : \"20\"\n",
    "}\n",
    "r = requests.post(\"http://httpbin.org/post\",data=data)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "成功返回了POST请求信息，其中form部分就是提交的数据data，表面POST请求成功发送了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cookies\": {\n",
      "    \"number\": \"123456789\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "s = requests.Session()\n",
    "s.get('http://httpbin.org/cookies/set/number/123456789')#设置一个cookie,名称叫作 number,内容是 123456789\n",
    "response = s.get('http://httpbin.org/cookies')  #再次请求了该网址，获取当前的 Cookies。\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "成功获取到设置的Cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.baidu.com  该网站下的子网站个数12\n",
      "http://jianyi.baidu.com/  该网站下的子网站个数8\n",
      "http://rj.baidu.com/soft/detail/11843.html?ald  该网站下的子网站个数13\n",
      "http://www.baidu.com/link?url=01vNBVXR2eaJxETl9PI3hcrvKCcwaJIKk5FkpO7l5YI_Q_pC24ogIBoZxI0LAu5oYpAdhRH42nzxAqfui0YnHK  该网站下的子网站个数218\n",
      "http://index.baidu.com  该网站下的子网站个数3\n"
     ]
    }
   ],
   "source": [
    "import requests,re\n",
    "count = 5  #控制站点层数\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')  #把正则表达式编译成正则表达对象，方便复用\n",
    "seed = 'https://www.baidu.com'\n",
    "stack = [seed]  #用栈数据结构来深度遍历\n",
    "while len(stack)>0 and count>0:\n",
    "    try:\n",
    "        url = stack.pop(-1)  #取栈顶元素(该层网页最后一个url)\n",
    "        html = requests.get(url).text #获取get请求得到的html原文本\n",
    "        new_urls = r.findall(html) #找该站点所有符合设定正则形式的url\n",
    "        stack.extend(new_urls) #压栈\n",
    "        number = len(new_urls)\n",
    "        count = count - 1\n",
    "        print(url+ \"  该网站下的子网站个数\"+ \"%d\"  %number)\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://httpbin.org/ 4\n",
      "https://fonts.googleapis.com/css?family=Open+Sans:400,700|Source+Code+Pro:300,600|Titillium+Web:400,600,700 0\n",
      "https://github.com/requests/httpbin 55\n",
      "https://kennethreitz.org 45\n",
      "https://github.com/rochacbruno/flasgger 30\n"
     ]
    }
   ],
   "source": [
    "#去重算法\n",
    "import requests,re\n",
    "count = 5 #控制遍历层数\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/'\n",
    "queue = [seed]\n",
    "used = set() #用集合特点模拟hash表\n",
    "storage = {}\n",
    "while len(queue)>0 and count>0:\n",
    "    try:\n",
    "        new_url_ =[]  #用来计数当前层去重后的url个数\n",
    "        url = queue.pop(0)  #取队列中首次访问的url\n",
    "        used.add(url)\n",
    "        html = requests.get(url).text\n",
    "        new_urls = r.findall(html)\n",
    "        for new_url in new_urls: #遍历该层所有站点，将新发现的且不相同的url加入队列中\n",
    "            if new_url not in used and new_url not in queue:\n",
    "                queue.append(new_url)\n",
    "                new_url_.append(new_url)\n",
    "        count = count - 1\n",
    "        print(url,len(new_url_))  #输出与该站点“平行”（同层）且互异的网站个数\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://httpbin.org/ 4\n",
      "https://fonts.googleapis.com/css?family=Open+Sans:400,700|Source+Code+Pro:300,600|Titillium+Web:400,600,700 0\n",
      "https://github.com/requests/httpbin 59\n",
      "https://kennethreitz.org 51\n",
      "https://github.com/rochacbruno/flasgger 71\n"
     ]
    }
   ],
   "source": [
    "#不去重\n",
    "import requests,re\n",
    "count = 5 #控制遍历层数\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/'\n",
    "queue = [seed]\n",
    "used = set()\n",
    "while len(queue)>0 and count>0:\n",
    "    try:\n",
    "        url = queue.pop(0)  #取队列中的起始url （遍历各层网页时首次访问的url）\n",
    "        used.add(url)\n",
    "        html = requests.get(url).text\n",
    "        new_urls = r.findall(html)\n",
    "        queue.extend(new_urls) #将处于与当前访问站点相同层数的站点加入到队列中（不论是否相同）\n",
    "        print(url,len(new_urls))  #输出与当前访问站点“平行”（同层）（有可能重复）的网站个数\n",
    "        count = count - 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seaching():\n",
    "    def DFS_Url(url):\n",
    "        '''\n",
    "        输入一个种子网站，进行去重深度遍历\n",
    "        '''\n",
    "        count = 5  #控制站点层数\n",
    "        r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')  #把正则表达式编译成正则表达对象，方便复用\n",
    "        seed = url\n",
    "        stack = [seed]  #用栈数据结构来深度遍历\n",
    "        used = set()\n",
    "        while len(stack)>0 and count>0:\n",
    "            try:\n",
    "                new_url_ =[] #计数当前层去重后的url个数\n",
    "                url = stack.pop(-1)  #取栈顶元素(该层网页最后一个url)\n",
    "                used.add(url)\n",
    "                html = requests.get(url).text #获取get请求得到的html原文本\n",
    "                new_urls = r.findall(html) #找该站点所有符合设定正则形式的url\n",
    "                for new_url in new_urls:\n",
    "                    if new_url not in used and new_url not in stack:\n",
    "                        stack.append(new_url)\n",
    "                number = len(new_urls)\n",
    "                count = count - 1\n",
    "                print(url+ \"  该网站下的子网站个数\"+ \"%d\"  %number)\n",
    "            except Exception as e:\n",
    "                print(url)\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
