{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名+作业序号，示例：``1_20188989899_张三_1``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.request``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 我电脑上的jupyter notebook 出问题了，不能正常解释，所以程序是在ipython中\n",
    "#     执行的，后面也有完整的程序\n",
    "In [1]: import urllib.request\n",
    "\n",
    "In [2]: url = 'http://httpbin.org/get'\n",
    "\n",
    "In [3]: response = urllib.request.urlopen(url).read()\n",
    "\n",
    "In [4]: print(response)\n",
    "b'{\\n  \"args\": {}, \\n  \"headers\": {\\n    \"Accept-Encoding\": \"identity\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"Python-urllib/3.7\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-5e65c0e5-809855d6ff83e300cd56f7dc\"\\n  }, \\n  \"origin\": \"117.136.2.197\", \\n  \"url\": \"http://httpbin.org/get\"\\n}\\n'\n",
    "\n",
    "In [5]: print(response.decode('utf-8'))\n",
    "{\n",
    "  \"args\": {},\n",
    "  \"headers\": {\n",
    "    \"Accept-Encoding\": \"identity\",\n",
    "    \"Host\": \"httpbin.org\",\n",
    "    \"User-Agent\": \"Python-urllib/3.7\",\n",
    "    \"X-Amzn-Trace-Id\": \"Root=1-5e65c0e5-809855d6ff83e300cd56f7dc\"\n",
    "  },\n",
    "  \"origin\": \"117.136.2.197\",\n",
    "  \"url\": \"http://httpbin.org/get\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = 'http://httpbin.org/get'\n",
    "response = urllib.request.urlopen(url).read()\n",
    "print(response)\n",
    "print(response.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用urllib.request库的urllib.request.urlopen()函数对网站进行GET请求，使用.decode()方法对响应数据进行编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [6]: import requests\n",
    "\n",
    "In [7]: url = \"http://httpbin.org/post\"\n",
    "\n",
    "In [8]: headers = {\n",
    "   ...:     \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
    "   ...:     \"host\":'httpbin.org'\n",
    "   ...: }\n",
    "\n",
    "In [9]: post_response = requests.post(url,headers=headers,data=None,json=None)\n",
    "\n",
    "In [10]: print(post_response)\n",
    "<Response [200]>\n",
    "\n",
    "In [13]: print(post_response.text)\n",
    "{\n",
    "  \"args\": {},\n",
    "  \"data\": \"\",\n",
    "  \"files\": {},\n",
    "  \"form\": {},\n",
    "  \"headers\": {\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Content-Length\": \"0\",\n",
    "    \"Host\": \"httpbin.org\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"X-Amzn-Trace-Id\": \"Root=1-5e65c73c-5ba5faba0933d0f24fe7845f\"\n",
    "  },\n",
    "  \"json\": null,\n",
    "  \"origin\": \"117.136.2.197\",\n",
    "  \"url\": \"http://httpbin.org/post\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"http://httpbin.org/post\"\n",
    "headers = {\n",
    "    \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
    "    \"host\":'httpbin.org'\n",
    "}\n",
    "post_response = requests.post(url,headers=headers,data=None,json=None)\n",
    "print(post_response)\n",
    "print(post_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用requests库的request.post()函数对网站进行POST请求，利用headers参数设置浏览器信息伪装成一个火狐浏览器，得到响应信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [6]: import requests\n",
    "\n",
    "In [15]: s = requests.Session()\n",
    "\n",
    "In [16]: s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')\n",
    "Out[16]: <Response [200]>\n",
    "\n",
    "In [17]: r = s.get(\"http://httpbin.org/cookies\")\n",
    "\n",
    "In [18]: r.text\n",
    "Out[18]: '{\\n  \"cookies\": {\\n    \"sessioncookie\": \"123456789\"\\n  }\\n}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "s = requests.Session()\n",
    "s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')\n",
    "r = s.get(\"http://httpbin.org/cookies\")\n",
    "r.text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 利用requests库创建一个session对象\n",
    "* 用这个session对象发出get请求，设置cookies\n",
    "* 用session对象发出另外一个get请求，获取cookies\n",
    "* 最终显示结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]: import re,requests\n",
    "\n",
    "In [2]: count = 5\n",
    "\n",
    "In [3]: r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "\n",
    "In [4]: seed = 'http://httpbin.org'\n",
    "\n",
    "In [5]: stack = [seed]\n",
    "\n",
    "In [6]: storage = {}\n",
    "\n",
    "In [7]: while len(stack)>0 and count>0:\n",
    "   ...:     try:\n",
    "   ...:         url = stack.pop(-1)\n",
    "   ...:         html = requests.get(url).text\n",
    "   ...:         new_urls = r.findall(html)\n",
    "   ...:         stack.extend(new_urls)\n",
    "   ...:         storage[url] = len(new_urls)\n",
    "   ...:         count -= 1\n",
    "   ...:     except Exception as e:\n",
    "   ...:         print(url)\n",
    "   ...:         print(e)\n",
    "   ...:\n",
    "\n",
    "In [8]: len(stack)\n",
    "Out[8]: 202\n",
    "\n",
    "In [9]: stack[1]\n",
    "Out[9]: 'https://github.com/requests/httpbin'\n",
    "\n",
    "In [10]: stack[-1]\n",
    "Out[10]: 'https://github.com/github'\n",
    "    \n",
    "In [11]: stack0 = set(stack)\n",
    "\n",
    "In [12]: len(stack0)\n",
    "Out[12]: 85    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,requests\n",
    "count = 5\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' \n",
    "stack = [seed]\n",
    "storage = {}\n",
    "while len(stack) > 0 and count > 0:\n",
    "    try:\n",
    "        url = stack.pop(-1)\n",
    "        html = requests.get(url).text\n",
    "        new_urls = r.findall(html)\n",
    "        stack.extend(new_urls)\n",
    "        storage[url] = len(new_urls)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)\n",
    "print(len(stack))  \n",
    "stack0 = set(stack)\n",
    "print(len(stack0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用list模拟栈,实现网页爬取的DFS算法，不包含去重的操作，当list转化成set时，长度减少了很多，证明包含了很多重复的URL。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [18]: # 不去重的广度优先\n",
    "\n",
    "In [19]: import re,requests\n",
    "\n",
    "In [20]: count = 10\n",
    "\n",
    "In [21]: r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "\n",
    "In [22]: seed = 'http://httpbin.org'\n",
    "\n",
    "In [23]: queue = [seed]\n",
    "\n",
    "In [24]: storage = {}\n",
    "\n",
    "In [25]: while len(queue)>0 and count>0:\n",
    "    ...:     try:\n",
    "    ...:         url = queue.pop(0)\n",
    "    ...:         html = requests.get(url).text\n",
    "    ...:         new_urls = r.findall(html)\n",
    "    ...:         queue.extend(new_urls)\n",
    "    ...:         storage[url] = len(new_urls)\n",
    "    ...:         count -= 1\n",
    "    ...:     except Exception as e:\n",
    "    ...:         print(url)\n",
    "    ...:         print(e)\n",
    "    ...:\n",
    "\n",
    "In [27]: len(queue)\n",
    "Out[27]: 400\n",
    "    \n",
    "In [28]: queue[1]\n",
    "Out[28]: 'https://user-images.githubusercontent.com/'\n",
    "\n",
    "In [29]: queue[-1]\n",
    "Out[29]: 'https://github.com/github'\n",
    "\n",
    "In [30]: queue0 = set(queue)\n",
    "\n",
    "In [31]: len(queue0)\n",
    "Out[31]: 146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' \n",
    "queue = [seed]\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = queue.pop(0)\n",
    "        html = requests.get(url).text\n",
    "        new_urls = r.findall(html)\n",
    "        queue.extend(new_urls)\n",
    "        storage[url] = len(new_urls)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e) \n",
    "print(len(queue))\n",
    "queue0 = set(queue)\n",
    "print(len(queue0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析：\n",
    "利用list模拟队列，实现网页爬取的BFS算法，不包含去重，当list转化成set时，长度减少了很多，证明包含了很多重复的URL。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 含去重操作\n",
    "In [1]: import requests, re\n",
    "\n",
    "In [2]: count = 5\n",
    "\n",
    "In [3]: r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "\n",
    "In [4]: seed = 'http://httpbin.org/'\n",
    "\n",
    "In [5]: queue = [seed]\n",
    "\n",
    "In [6]: used = set() # 设置一个集合，保存已经抓取过的 URL\n",
    "\n",
    "In [7]: storage = {}\n",
    "\n",
    "In [8]: while len(queue) > 0 and count > 0:\n",
    "   ...:     try:\n",
    "   ...:         url = queue.pop(0)\n",
    "   ...:         html = requests.get(url).text\n",
    "   ...:         storage[url] = html\n",
    "   ...:         used.add(url) # 将已经抓取过的URL存入used集合中\n",
    "   ...:         new_urls = r.findall(html)\n",
    "   ...:         for new_url in new_urls: # 将新发新未抓取的URL添加到queue中\n",
    "   ...:             if new_url not in used and new_url not in queue:\n",
    "   ...:                 queue.append(new_url)\n",
    "   ...:         count -= 1\n",
    "   ...:     except Exception as e:\n",
    "   ...:         print(url)\n",
    "   ...:         print(e)\n",
    "   ...:\n",
    "\n",
    "In [9]:\n",
    "\n",
    "In [9]: len(queue)\n",
    "Out[9]: 130\n",
    "\n",
    "In [11]: len(used)\n",
    "Out[11]: 5\n",
    "\n",
    "In [12]: queue0 = set(queue)\n",
    "\n",
    "In [13]: len(queue0)\n",
    "Out[13]: 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "count = 5\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' \n",
    "queue = [seed]\n",
    "used = set() \n",
    "storage = {}\n",
    "\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = queue.pop(0)\n",
    "        html = requests.get(url).text\n",
    "        storage[url] = html\n",
    "        used.add(url) \n",
    "        new_urls = r.findall(html)\n",
    "        for new_url in new_urls:\n",
    "            if new_url not in used and new_url not in queue:\n",
    "                queue.append(new_url)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用list模拟队列，实现网页爬取的BFS算法，包含去重，当list转化成set时，长度不变，证明不包含重复的URL。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import requests\n",
    "try:\n",
    "    import _locale\n",
    "except ImportError:\n",
    "    _locale = None\n",
    "\n",
    "__version__ = \"0.0.1\"\n",
    "\n",
    "__r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)') # 获取http链接的正则\n",
    "\n",
    "def findurl(url,n=5):\n",
    "    '''\n",
    "    返回从URL指定网址爬取的URL链接组成的列表变量，采用深度优先爬取算法。\n",
    "    \n",
    "    | url ： 指定的待爬取网址链接\n",
    "    | n   ： 指定的爬取深度，默认深度为5\n",
    "    '''\n",
    "    stack = [url]\n",
    "    used = set() # 利用集合的互异性判断重复的URL，最终达到去重的目的\n",
    "    storage = {}\n",
    "    while len(stack) > 0 and n > 0:\n",
    "        try:\n",
    "            seed = stack.pop(-1)\n",
    "            html = requests.get(seed).text\n",
    "            storage[seed] = html\n",
    "            used.add(seed)\n",
    "            new_urls = __r.findall(html)\n",
    "            for new_url in new_urls:\n",
    "                if new_url not in used and new_url not in stack:\n",
    "                    stack.append(new_url)\n",
    "            n -= 1\n",
    "        except Exception as e:\n",
    "            print(seed)\n",
    "            print(e)\n",
    "    return stack\n",
    "if __name__ == '__main__':\n",
    "    url = 'http://httpbin.org/'\n",
    "    list_test = findurl(url)\n",
    "    print('从网址 “http://httpbin.org” 利用深度优先算法，深度为5时，爬取得到的url列表有 %d 个元素' % len(list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [16]: import dfs_wx\n",
    "    \n",
    "In [17]: dfs_wx.findurl?\n",
    "Signature: dfs_wx.findurl(url, n=5)\n",
    "Docstring:\n",
    "返回从URL指定网址爬取的URL链接组成的列表变量，采用深度优先爬取算法。\n",
    "\n",
    "| url ： 指定的待爬取网址链接\n",
    "| n   ： 指定的爬取深度，默认深度为5\n",
    "File:      c:\\users\\administrator\\desktop\\dfs_wx.py\n",
    "Type:      function\n",
    "\n",
    "In [18]: url = 'http://httpbin.org/'\n",
    "\n",
    "In [19]: test = dfs_wx.findurl(url)\n",
    "\n",
    "In [20]: len(test)\n",
    "Out[20]: 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 暂时只封装了获取URL的函数，功能十分单一\n",
    "* 模块封装还很不规范\n",
    "* 缺少内存释放等机制\n",
    "* ……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的就不用看了，我作死的把99个链接都打印出来了……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [24]: for each in test:\n",
    "    ...:     print(each)\n",
    "    ...:\n",
    "https://fonts.googleapis.com/css?family=Open+Sans:400,700|Source+Code+Pro:300,600|Titillium+Web:400,600,700\n",
    "https://github.com/requests/httpbin\n",
    "https://kennethreitz.org\n",
    "https://github.githubassets.com\n",
    "https://avatars0.githubusercontent.com\n",
    "https://avatars1.githubusercontent.com\n",
    "https://avatars2.githubusercontent.com\n",
    "https://avatars3.githubusercontent.com\n",
    "https://github-cloud.s3.amazonaws.com\n",
    "https://user-images.githubusercontent.com/\n",
    "https://github.githubassets.com/assets/frameworks-faf8983e496d05877e58c5b73e184080.css\n",
    "https://github.githubassets.com/assets/site-db5797d9868460861af755858749976b.css\n",
    "https://github.githubassets.com/assets/github-fa6b2a43748bf3916c3c1996db6fc5fa.css\n",
    "https://github.com/fluidicon.png\n",
    "https://github.githubassets.com/\n",
    "https://github.com/flasgger/flasgger/commits/master.atom\n",
    "https://github.com/flasgger/flasgger\n",
    "https://github.githubassets.com/pinned-octocat.svg\n",
    "https://github.githubassets.com/favicon.ico\n",
    "https://github.com/\n",
    "https://lab.github.com/\n",
    "https://opensource.guide\n",
    "https://github.com/events\n",
    "https://github.community\n",
    "https://education.github.com\n",
    "https://enterprise.github.com/contact\n",
    "http://flasgger.pythonanywhere.com/\n",
    "https://help.github.com/articles/which-remote-url-should-i-use\n",
    "https://desktop.github.com\n",
    "https://desktop.github.com/\n",
    "https://developer.apple.com/xcode/\n",
    "https://visualstudio.github.com/\n",
    "https://github.com/flasgger/flasgger/commit/083cd152941b27ae20f6fcbf1c467fc2dea95587\n",
    "https://github.com/flasgger/flasgger/issues/366\n",
    "https://github.com/flasgger/flasgger/pull/149\n",
    "https://github.com/flasgger/flasgger/pull/150\n",
    "https://github.com/flasgger/flasgger/issues/194\n",
    "https://github.com/flasgger/flasgger/issues/317\n",
    "https://travis-ci.com/flasgger/flasgger\n",
    "https://landscape.io/github/rochacbruno/flasgger/master\n",
    "https://coveralls.io/github/rochacbruno/flasgger?branch=master\n",
    "https://pypi.python.org/pypi/flasgger\n",
    "https://www.paypal.com/cgi-bin/webscr?cmd=_donations&amp;business=rochacbruno%40gmail%2ecom&amp;lc=BR&amp;item_name=Flasgger&amp;no_note=0&amp;currency_code=USD&amp;bn=PP%2dDonationsBF%3abtn_donate_SM%2egif%3aNonHostedGuest\n",
    "https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#operation-object\n",
    "http://swagger.io/swagger-ui/\n",
    "https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/0\n",
    "https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/1\n",
    "https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/2\n",
    "https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/3\n",
    "https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/4\n",
    "https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/5\n",
    "https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/6\n",
    "https://sourcerer.io/fame/rochacbruno/rochacbruno/flasgger/links/7\n",
    "http://localhost:5000\n",
    "http://localhost:5000/apidocs/\n",
    "https://github.com/rochacbruno/flasgger/blob/aaef05c17cc559d01b7436211093463642eb6ae2/examples/parsed_view_func.py#L16\n",
    "http://json-schema.org/latest/json-schema-validation.html\n",
    "https://python-jsonschema.readthedocs.io/en/latest/\n",
    "http://flask.pocoo.org/snippets/35/\n",
    "https://github.com/site/terms\n",
    "https://github.com/site/privacy\n",
    "https://github.com/security\n",
    "https://githubstatus.com/\n",
    "https://help.github.com\n",
    "https://github.com\n",
    "https://github.com/contact\n",
    "https://github.com/pricing\n",
    "https://developer.github.com\n",
    "https://training.github.com\n",
    "https://github.blog\n",
    "https://socialimpact.github.com/\n",
    "https://www.twitter.com/github\n",
    "https://resources.github.com\n",
    "http://partner.github.com/\n",
    "https://atom.io\n",
    "http://electronjs.org\n",
    "https://services.github.com/\n",
    "https://shop.github.com\n",
    "https://twitter.com/github\n",
    "https://www.facebook.com/GitHub\n",
    "https://www.youtube.com/github\n",
    "https://www.linkedin.com/company/github\n",
    "https://help.github.com/en/github/setting-up-and-managing-organizations-and-teams/verifying-your-organizations-domain\n",
    "https://help.github.com/cn/github/setting-up-and-managing-organizations-and-teams/verifying-your-organizations-domain\n",
    "https://help.github.com/ja/github/setting-up-and-managing-organizations-and-teams/verifying-your-organizations-domain\n",
    "https://help.github.com/es/github/setting-up-and-managing-organizations-and-teams/verifying-your-organizations-domain\n",
    "https://help.github.com/pt/github/setting-up-and-managing-organizations-and-teams/verifying-your-organizations-domain\n",
    "https://help.github.com/de/github/setting-up-and-managing-organizations-and-teams/verifying-your-organizations-domain\n",
    "https://atom.io/docs\n",
    "https://electronjs.org/docs\n",
    "https://support.github.com/contact\n",
    "https://github.com/features\n",
    "https://github.com/enterprise\n",
    "https://github.com/case-studies?type=customers\n",
    "https://developer.github.com/\n",
    "http://electron.atom.io/\n",
    "https://github.blog/\n",
    "https://github.com/about/careers\n",
    "https://github.com/about/press"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
