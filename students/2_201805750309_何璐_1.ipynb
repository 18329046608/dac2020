{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名+作业序号，示例：``1_20188989899_张三_1``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"word\": \"hello\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"10\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python-urllib/3.7\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e66739c-8442c5f8bb1f07316c887110\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"110.53.234.42\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "data = urllib.parse.urlencode({'word': 'hello'}).encode()#get请求的参数\n",
    "response = urllib.request.urlopen('http://httpbin.org/post',data=data)#发送请求\n",
    "print(response.read().decode())#将得到的响应解码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"headers\"代表请求头**\n",
    "\n",
    "Accept-Encoding：客户端支持的压缩方式\n",
    "\n",
    "Content-Length：消息体的长度为10\n",
    "\n",
    "Content-Type：客户端告诉服务器实际发送的数据类型\n",
    "\n",
    "Host：要请求的服务器域名，与url一致\n",
    "\n",
    "User-Agent客户端标识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {\n",
      "    \"name\": \"Hackdata\"\n",
      "  }, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"0\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e667e45-dc6bece86fb204b8bc0fcc14\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"110.53.234.42\", \n",
      "  \"url\": \"http://httpbin.org/post?name=Hackdata\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests,re\n",
    "url=\"http://httpbin.org/post\"\n",
    "headers={\n",
    "    \"User-Agent\":'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',\n",
    "    \"host\":'httpbin.org'\n",
    "         }#伪装一个火狐浏览器\n",
    "dict={\"name\":\"Hackdata\"}\n",
    "response=requests.post(url,params=dict,headers=headers)#发起post请求\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"headers\"后为请求头**\n",
    "\n",
    "Accept:* / * 表示客户端支持任意的MIME类型\n",
    "\n",
    "Accept-Encoding: 客户端支持gzip, deflate这两种压缩方式\n",
    "\n",
    "Host:要请求的服务器域名\"httpbin.org\"\n",
    "\n",
    "User-Agent:客户端标识\n",
    "\n",
    "X-Amzn-Trace-Id\":可记录此唯一标识符，以解决与负载均衡器相关的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cookies\": {\n",
      "    \"sessioncookie\": \"123456789\"\n",
      "  }\n",
      "}\n",
      "\n",
      "{'User-Agent': 'python-requests/2.22.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Cookie': 'sessioncookie=123456789'}\n"
     ]
    }
   ],
   "source": [
    "s = requests.Session() # 创建一个session对象 \n",
    "s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')\n",
    "                                        # 用session对象发出get请求，设置cookies \n",
    "r = s.get(\"http://httpbin.org/cookies\") # 用session对象发出另外一个get请求，获取cookies \n",
    "print(r.text)# 显示结果 \n",
    "print(r.request.headers)#查看发送请求的请求头"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests库的session对象能够帮我们跨请求保持某些参数，也会在同一个session实例发出的所有请求之间保持cookies。\n",
    "\n",
    "Cookie:在客户端访问Web服务器时，服务器在客户端硬盘上存放的信息\n",
    "\n",
    "当客户端首次请求访问服务器时，服务器先在客户端存放包含该客户的相关信息的Cookie，以后客户端每次请求访问服务器时，都会在HTTP请求数据中包含Cookie，服务器解析HTTP请求中的Cookie，就能由此获得关于客户的相关信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/%E7%B4%AB%E8%8B%8F/511', '/item/%E8%8B%8F%E5%AD%90%E6%B2%B9/8682670', '/item/%E8%8D%89%E6%9C%AC', '/item/%E6%80%BB%E7%8A%B6%E8%8A%B1%E5%BA%8F', '/item/%E8%8B%9E%E7%89%87', '/item/%E6%8A%AB%E9%92%88%E5%BD%A2', '/item/%E5%9D%9A%E6%9E%9C/80641', '/item/%E7%B4%AB%E8%8B%8F/511', '/item/%E9%87%8E%E7%94%9F%E7%B4%AB%E8%8B%8F', '/item/%E8%80%B3%E9%BD%BF%E5%8F%98%E7%A7%8D', '/item/%E5%9B%9E%E5%9B%9E%E8%8B%8F', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n",
      "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E8%BA%AB%E4%BB%BD%E8%AF%81/113951', '/item/%E6%88%B7%E5%8F%A3%E6%9C%AC/5104621', '/item/%E4%B9%89%E9%A1%B9/6176882', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n",
      "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E8%BA%AB%E4%BB%BD%E8%AF%81/113951', '/item/%E6%88%B7%E5%8F%A3%E6%9C%AC/5104621', '/item/%E4%B9%89%E9%A1%B9/6176882', '/item/%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/5718004', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n",
      "/item/%E7%B4%AB%E8%8B%8F%E5%B1%9E 0\n",
      "/item/秒懂大师说 1\n",
      "/item/秒懂大师说\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂看瓦特 1\n",
      "/item/秒懂看瓦特\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/%E6%80%BB%E7%8A%B6%E8%8A%B1%E5%BA%8F 1\n",
      "/item/%E6%8A%AB%E9%92%88%E5%BD%A2 1\n",
      "/item/%E8%80%B3%E9%BD%BF%E5%8F%98%E7%A7%8D 1\n",
      "/item/%E7%B4%AB%E8%8B%8F/511 1\n",
      "/item/秒懂五千年 1\n",
      "/item/秒懂五千年\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/%E5%9B%9E%E5%9B%9E%E8%8B%8F 1\n",
      "/item/%E8%8B%9E%E7%89%87 1\n",
      "/item/%E8%8D%89%E6%9C%AC 1\n",
      "/item/秒懂全视界 1\n",
      "/item/秒懂全视界\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂星课堂 1\n",
      "/item/秒懂星课堂\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/%E9%87%8E%E7%94%9F%E7%B4%AB%E8%8B%8F 1\n",
      "/item/%E5%9D%9A%E6%9E%9C/80641 1\n",
      "/item/%E8%8B%8F%E5%AD%90%E6%B2%B9/8682670 1\n",
      "/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter 1\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as ur\n",
    "import re\n",
    "r = ur.urlopen(\"https://baike.baidu.com/item/\"\n",
    "               \"%E7%B4%AB%E8%8B%8F%E5%B1%9E\") # 对百度百科关键词条为紫苏属的URL的进行访问\n",
    "content = r.read().decode('utf-8')\n",
    "href = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)') # 利用正则表达式将网页中所需的链接表达出来\n",
    "new_urls=href.findall(content) # 使用findall方法将所有链接信息抽取出来\n",
    "#print(new_urls) 打印该网页中的所有链接\n",
    "\n",
    "#实现深度优先爬取\n",
    "count = 0\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)') # 抽取所需链接信息的正则语言规则\n",
    "seed = \"/item/%E7%B4%AB%E8%8B%8F%E5%B1%9E\" # 这是紫苏属词条\n",
    "stack = [seed] # 设置种子链接的栈（使用列表模拟栈）\n",
    "storage = {}\n",
    "while count < 3:\n",
    "    try:\n",
    "        url = stack.pop(-1) # 取出栈的最后一条URL\n",
    "        html = ur.urlopen(\"https://baike.baidu.com\"+url).read().decode('utf-8') # 对URL进行拼接\n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)\n",
    "        stack.extend(new_urls) # 将新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "#加层数控制的\n",
    "count = 0 # 层数\n",
    "floors = 1 # 限制爬取的层数\n",
    "lastStep = []\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)')\n",
    "seed = '/item/%E7%B4%AB%E8%8B%8F%E5%B1%9E' # 这是紫苏属词条\n",
    "queue = [[seed]]\n",
    "for i in range(floors): # 限制爬取范围在设定的层数范围内\n",
    "    queue.append([])\n",
    "storage = {}\n",
    "used = set() # 设置集合存放爬取过的url\n",
    "while len(queue[0])>0 or count!=0: # 种子队列不为空或者层数不为零\n",
    "    try:\n",
    "        url = queue[count].pop(-1)\n",
    "        print(url+\" \"+str(count)) # 打印当前链接和层数\n",
    "        html = ur.urlopen('https://baike.baidu.com'+url).read().decode('utf-8')\n",
    "        storage[url]=html\n",
    "        used.add(url) # 将爬取过的URL放入集合中\n",
    "        new_urls = r.findall(html)\n",
    "        if count < floors:\n",
    "            for new_url in set(new_urls):\n",
    "                if new_url not in used and new_url not in queue:#判断新链接网址中的包含的链接是否为重复的\n",
    "                    queue[count+1].append(new_url) # 将爬取的URL存入到队列中相应层数的列表\n",
    "            count+=1\n",
    "        else:\n",
    "            if len(queue[count])==0:\n",
    "                count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果看出，由于网页的最后一个链接都是相同的，所以在第一次爬取后再爬取的所有信息都将是同一个链接，即爬取的是重复信息，而且爬虫将陷入无限循环。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/%E7%B4%AB%E8%8B%8F/511', '/item/%E8%8B%8F%E5%AD%90%E6%B2%B9/8682670', '/item/%E8%8D%89%E6%9C%AC', '/item/%E6%80%BB%E7%8A%B6%E8%8A%B1%E5%BA%8F', '/item/%E8%8B%9E%E7%89%87', '/item/%E6%8A%AB%E9%92%88%E5%BD%A2', '/item/%E5%9D%9A%E6%9E%9C/80641', '/item/%E7%B4%AB%E8%8B%8F/511', '/item/%E9%87%8E%E7%94%9F%E7%B4%AB%E8%8B%8F', '/item/%E8%80%B3%E9%BD%BF%E5%8F%98%E7%A7%8D', '/item/%E5%9B%9E%E5%9B%9E%E8%8B%8F', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n",
      "/item/秒懂星课堂\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂大师说\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂看瓦特\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂五千年\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂全视界\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "['/item/秒懂星课堂', '/item/秒懂大师说', '/item/秒懂看瓦特', '/item/秒懂五千年', '/item/秒懂全视界', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E5%A4%9A%E4%B9%89%E8%AF%8D', '/item/%E4%B9%89%E9%A1%B9', '/item/%E7%B4%AB%E8%8B%8F?force=1', '/item/%E7%B4%AB%E8%8B%8F/2130894#viewPageContent', '/item/%E7%B4%AB%E8%8B%8F/10041549#viewPageContent', '/item/%E7%B4%AB%E8%8B%8F/2130860#viewPageContent', '/item/%E7%B4%AB%E8%8B%8F/2130876#viewPageContent', '/item/%E7%B4%AB%E8%8B%8F/17653933#viewPageContent', '/item/%E7%B4%AB%E8%8B%8F/24196985#viewPageContent', '/item/%E5%94%87%E5%BD%A2%E7%A7%91/2381957', '/item/%E8%8D%89%E6%9C%AC%E6%A4%8D%E7%89%A9', '/item/%E8%9B%A4%E8%92%8C/3618734', '/item/%E8%A2%AB%E5%AD%90%E6%A4%8D%E7%89%A9%E9%97%A8', '/item/%E5%8F%8C%E5%AD%90%E5%8F%B6%E6%A4%8D%E7%89%A9%E7%BA%B2', '/item/%E8%8F%8A%E4%BA%9A%E7%BA%B2', '/item/%E5%94%87%E5%BD%A2%E7%9B%AE', '/item/%E5%94%87%E5%BD%A2%E7%A7%91', '/item/%E9%87%8E%E8%8A%9D%E9%BA%BB%E4%BA%9A%E7%A7%91', '/item/%E5%A1%94%E8%8A%B1%E6%97%8F', '/item/%E7%B4%AB%E8%8B%8F%E5%B1%9E', '/item/%E9%A6%99%E6%96%99', '/item/%E8%8D%89%E6%9C%AC%E6%A4%8D%E7%89%A9', '/item/%E8%BD%AE%E4%BC%9E%E8%8A%B1%E5%BA%8F', '/item/%E5%A3%A4%E5%9C%9F', '/item/%E9%BB%8F%E5%A3%A4%E5%9C%9F', '/item/%E6%9D%8E%E6%97%B6%E7%8F%8D/80855', '/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter']\n",
      "/item/%E7%B4%AB%E8%8B%8F%E5%B1%9E 0\n",
      "/item/秒懂大师说 1\n",
      "/item/秒懂大师说\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂看瓦特 1\n",
      "/item/秒懂看瓦特\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/%E6%80%BB%E7%8A%B6%E8%8A%B1%E5%BA%8F 1\n",
      "/item/%E6%8A%AB%E9%92%88%E5%BD%A2 1\n",
      "/item/%E8%80%B3%E9%BD%BF%E5%8F%98%E7%A7%8D 1\n",
      "/item/%E7%B4%AB%E8%8B%8F/511 1\n",
      "/item/秒懂五千年 1\n",
      "/item/秒懂五千年\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/%E5%9B%9E%E5%9B%9E%E8%8B%8F 1\n",
      "/item/%E8%8B%9E%E7%89%87 1\n",
      "/item/%E8%8D%89%E6%9C%AC 1\n",
      "/item/秒懂全视界 1\n",
      "/item/秒懂全视界\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/秒懂星课堂 1\n",
      "/item/秒懂星课堂\n",
      "'ascii' codec can't encode characters in position 10-14: ordinal not in range(128)\n",
      "/item/%E9%87%8E%E7%94%9F%E7%B4%AB%E8%8B%8F 1\n",
      "/item/%E5%9D%9A%E6%9E%9C/80641 1\n",
      "/item/%E8%8B%8F%E5%AD%90%E6%B2%B9/8682670 1\n",
      "/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%EF%BC%9A%E6%9C%AC%E4%BA%BA%E8%AF%8D%E6%9D%A1%E7%BC%96%E8%BE%91%E6%9C%8D%E5%8A%A1/22442459?bk_fr=pcFooter 1\n"
     ]
    }
   ],
   "source": [
    "#或者使用广度优先爬取\n",
    "count = 0\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)') # 抽取所需链接信息的正则语言规则\n",
    "seed = \"/item/%E7%B4%AB%E8%8B%8F%E5%B1%9E\" # 这是紫苏属词条\n",
    "queue = [seed] # 设置种子链接的队列（使用列表模拟队列）\n",
    "storage = {}\n",
    "while count < 2:\n",
    "    try:\n",
    "        url = queue.pop(0) # 取出队列最后一条URL\n",
    "        html = ur.urlopen(\"https://baike.baidu.com\"+url).read().decode('utf-8') # 对URL进行拼接\n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)\n",
    "        queue.extend(new_urls) # 将新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)\n",
    "\n",
    "        \n",
    "#加层数控制的\n",
    "count = 0 # 层数\n",
    "floors = 1 # 限制爬取的层数\n",
    "lastStep = []\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)')\n",
    "seed = '/item/%E7%B4%AB%E8%8B%8F%E5%B1%9E' # 这是紫苏属词条\n",
    "queue = [[seed]]\n",
    "for i in range(floors): # 限制爬取范围在设定的层数范围内\n",
    "    queue.append([])\n",
    "storage = {}\n",
    "used = set() # 设置集合存放爬取过的url\n",
    "while len(queue[0])>0 or count!=0: # 种子队列不为空或者层数不为零\n",
    "    try:\n",
    "        url = queue[count].pop(-1)\n",
    "        print(url+\" \"+str(count)) # 打印当前链接和层数\n",
    "        html = ur.urlopen('https://baike.baidu.com'+url).read().decode('utf-8')\n",
    "        storage[url]=html\n",
    "        used.add(url) # 将爬取过的URL放入集合中\n",
    "        new_urls = r.findall(html)\n",
    "        if count < floors:\n",
    "            for new_url in set(new_urls):\n",
    "                if new_url not in used and new_url not in queue:#判断新链接网址中的包含的链接是否为重复的\n",
    "                    queue[count+1].append(new_url) # 将爬取的URL存入到队列中相应层数的列表\n",
    "            count+=1\n",
    "        else:\n",
    "            if len(queue[count])==0:\n",
    "                count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当网页链接中存在相同链接时，有必要对URL进行去重处理。即我们可以将爬取过的URL存放在一个元素集合中，在进行新的爬取之前将目标URL与爬取过的集合进行对比，只爬取元素集合中没有的URL，就可以完成去重处理了。并且设置限制爬取链接的层数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
