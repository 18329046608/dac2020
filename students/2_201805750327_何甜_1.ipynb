{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名，示例：``1_20188989899_张三``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python-urllib/3.7\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e660125-a904cc8825fc3a9046bf9710\"\n",
      "  }, \n",
      "  \"origin\": \"175.5.152.57\", \n",
      "  \"url\": \"http://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request                #导入urllib.request模块\n",
    "url_get = 'http://httpbin.org/get'      #相关网址\n",
    "response = urllib.request.urlopen(url_get)\n",
    "data = response.read().decode()          #使用read函数将response内容转移\n",
    "print(data)                  #输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出了网站的一些信息参数、header和来源，header中包含了host服务器域名、user-agent等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args': {}, 'data': '', 'files': {}, 'form': {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0'}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '295', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.22.0', 'X-Amzn-Trace-Id': 'Root=1-5e65ffce-3e355e9ecdeab66000ac3d8e'}, 'json': None, 'origin': '175.5.152.57', 'url': 'http://httpbin.org/post'}\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json     #导入模块\n",
    "\n",
    "#用requests 发起 post 请求\n",
    "url_post = 'http://httpbin.org/post' #网址\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0\",     #客户端标识\n",
    "        \"Connection\": \"keep-alive\",                                                                        #连接情况\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\", #客户端支持的MINE类型\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8\"}    #浏览器network中的header\n",
    "response = requests.post(url_post,headers)      #post请求\n",
    "print(response.json())                          #输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果中输出了网址在当前浏览器的network情况，输出内容基本在Request Headers中\n",
    "参考来自CSDN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cookies': {}}\n",
      "{'cookies': {'k1': 'v1', 'k2': 'v2'}}\n",
      "{'cookies': {'k1': 'v1', 'k2': 'v2'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json             #导入模块\n",
    "url_cookies = 'http://httpbin.org/cookies'    #网址\n",
    "url_set_cookies = 'http://httpbin.org/cookies/set?k1=v1&k2=v2'   #创建cookie\n",
    "session = requests.Session()     #session对象\n",
    "print (session.get(url_cookies, timeout=5).json())\n",
    "print (session.get(url_set_cookies, timeout=5).json())\n",
    "print (session.get(url_cookies, timeout=5).json())            #输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四行中有限定条件‘k1=v1&k2=v2'所以最后只能输出一个特定的值；但加上第五行之后，输出内容将会包含上述特定条件下的值，并输出其他可行的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/about\n",
      "HTTPSConnectionPool(host='github.com', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "import requests, re                 #导入模块 正则 爬虫\n",
    "count = 0                    #计数器\n",
    "seed = 'http://httpbin.org/'        #相应网址\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "stack = [seed]\n",
    "storage = {}\n",
    "while len(stack) > 0 and count < 10:           #限制条件  开始循环\n",
    "    try:\n",
    "        url = stack.pop(-1)      #输出栈的最后一条URL\n",
    "        html = requests.get(url, timeout=5).text       #get请求\n",
    "        new_urls = r.findall(html)\n",
    "        stack.extend(new_urls)                        #载入新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)                 #存储\n",
    "        count += 1                              #计数器 \n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)') # 抽取所需链接信息的正则语言规则\n",
    "seed = \"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB\" # 这是网络爬虫词条\n",
    "stack = [seed] # 设置种子链接的栈（使用列表模拟栈）\n",
    "storage = {}\n",
    "while count < 2:\n",
    "    try:\n",
    "        url = stack.pop(-1) # 取出栈的最后一条URL\n",
    "        html = ur.urlopen(\"https://baike.baidu.com\"+url).read().decode('utf-8') # 对URL进行拼接\n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)\n",
    "        stack.extend(new_urls) # 将新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个程序输出只有第一个显示正常，其余均显示ConnectTimeoutError(连接错误）通过修改局域网设置可以得到改善 输出了网址和timeout     \n",
    "第二个程序输出一些字符和pop from empty list (从空列表弹出) 可能是初始列表为空 无法进行接下来的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://avatars1.githubusercontent.com\n",
      "HTTPSConnectionPool(host='avatars1.githubusercontent.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(\"bad handshake: SysCallError(10054, 'WSAECONNRESET')\")))\n"
     ]
    }
   ],
   "source": [
    "import requests, re                      #导入模块 正则 爬虫\n",
    "count = 10                              #计数上限\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)')\n",
    "seed = 'http://httpbin.org/' #  网站\n",
    "queue = [seed]\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0:  #限制条件\n",
    "    try:\n",
    "        url = queue.pop(0)\n",
    "        html = requests.get(url, timeout=5).text\n",
    "        new_urls = r.findall(html)\n",
    "        queue.extend(new_urls)\n",
    "        storage[url] = len(new_urls)\n",
    "        count -= 1                  #计数递减\n",
    "    except Exception as e:\n",
    "        print (url)\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)') # 抽取所需链接信息的正则语言规则\n",
    "seed = \"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB\" # 这是网络爬虫词条\n",
    "queue = [seed] # 设置种子链接的队列（使用列表模拟队列）\n",
    "storage = {}\n",
    "while count < 2:\n",
    "    try:\n",
    "        url = queue.pop(0) # 取出队列最后一条URL\n",
    "        html = ur.urlopen(\"https://baike.baidu.com\"+url).read().decode('utf-8') # 对URL进行拼接\n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)\n",
    "        queue.extend(new_urls) # 将新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个程序输出来的网址中都因为SSLError（证书错误问题）而无法正常显示，参考CSDN资料中说可以通过修改局域网设置来解决 但我修改后没有解决这个问题   \n",
    "第二个程序输出了很多字符 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
