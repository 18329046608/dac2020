{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名+作业序号，示例：``1_20188989899_张三_1``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"args\": {}, \\n  \"headers\": {\\n    \"Accept-Encoding\": \"identity\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"Python-urllib/3.7\", \\n    \"X-Amzn-Trace-Id\": \"Root=1-5e634cfc-d410552bf66833e9f790d430\"\\n  }, \\n  \"origin\": \"60.10.17.92\", \\n  \"url\": \"http://httpbin.org/get\"\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "response = urllib.request.urlopen(\"http://httpbin.org/get\",timeout=1)\n",
    "print(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析：\n",
    "\n",
    "返回信息包括头部信息、客户端类型、客户端支持的压缩方式、要请求的服务器的域名等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"0\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/4.0(compatible;MSIE 5.5;Windows NT)\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e634a85-49bbb3edeeb6dd60e90e4a8f\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"60.10.17.92\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://httpbin.org/post'\n",
    "\n",
    "headers={ #伪装成一个火狐浏览器，加入头部信息\n",
    "    \"User-Agent\":'Mozilla/4.0(compatible;MSIE 5.5;Windows NT)',\n",
    "    \"host\":'httpbin.org'\n",
    "    }\n",
    "response = requests.post(url,headers = headers,timeout = 1)\n",
    "\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"cookies\": {\\n    \"sessioncookie\": \"123456789\"\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "s=requests.session()\n",
    "s.get('http://httpbin.org/cookies/set/sessioncookie/123456789') #用session对象发出get请求，设置cookies\n",
    "r=s.get(\"http://httpbin.org/cookies\") #用session对象发出另一个get请求，获取cookies\n",
    "r.text   #显示结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析:\n",
    "\n",
    "获取的cookies信息是自己设置好的，并且表现为字典形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,re\n",
    "count = 0\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)') # 正则表达式\n",
    "seed = \"http://httpbin.org/\" \n",
    "stack = [seed] # 设置种子链接的栈（使用列表模拟栈）\n",
    "storage = {}\n",
    "while count < 2:\n",
    "    try:\n",
    "        url = stack.pop(-1) # 取出栈的最后一条URL\n",
    "        html = ur.urlopen(\"http://httpbin.org/\"+url).read().decode('utf-8') # 对URL进行拼接\n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)\n",
    "        stack.extend(new_urls) # 将新提取的链接信息入栈\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析：\n",
    "\n",
    "存在大量重复网页信息，且数量极多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 去重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://education.github.com 4\n",
      "https://desktop.github.com/ 4\n",
      "https://github.githubassets.com 2\n",
      "https://avatars0.githubusercontent.com 2\n",
      "https://avatars1.githubusercontent.com 2\n"
     ]
    }
   ],
   "source": [
    "import requests,re\n",
    "count = 5\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\">]+)') #正则表达式\n",
    "seed = 'http://httpbin.org/'\n",
    "queue = [seed] # 设置种子链接的队列（使用列表模拟队列）\n",
    "storage = {}\n",
    "\n",
    "while len (queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = queue.pop(0)\n",
    "        html = requests.get(url).text\n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        queue.extend(new_urls) # 将新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)\n",
    "from collections import Counter  #计数\n",
    "url_count = Counter(queue)\n",
    "for url,count in url_count.most_common(5):\n",
    "    print(url,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不去重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,re\n",
    "count = 0\n",
    "r = re.compile(r'href=[\\'\"]?(/item[^\\'\" >]+)') # 正则表达式\n",
    "seed = \"http://httpbin.org/\" \n",
    "queue = [seed] # 设置种子链接的队列（使用列表模拟队列）\n",
    "storage = {}\n",
    "while count < 2:\n",
    "    try:\n",
    "        url = queue.pop(0) # 取出队列最后一条URL\n",
    "        html = ur.urlopen(\"https:httpbin.org\"+url).read().decode('utf-8') # 对URL进行拼接\n",
    "        new_urls = r.findall(html) # 提取当前网页下的所有链接URL信息\n",
    "        print(new_urls)\n",
    "        queue.extend(new_urls) # 将新提取的链接信息入队列\n",
    "        storage[url] = len(new_urls)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析：\n",
    "\n",
    "未去重前，存在大量重复信息，去重后，信息简单明了，网站数目可计。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
