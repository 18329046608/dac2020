{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《数据采集与清洗》\n",
    "## 第一次作业内容： 网页抓取\n",
    "### 具体目标：\n",
    "+ 用``urllib.requests``库发起一次``get``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息；\n",
    "+ 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容；\n",
    "+ 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果；\n",
    "+ 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果；\n",
    "+ (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。\n",
    "\n",
    "### 注：\n",
    "+ 代码要有注释，结果要有分析；\n",
    "+ 本次作业提交截至时间：2020年3月10日(星期二)；\n",
    "+ 文件命名规则: 班级号+学号+姓名，示例：``1_20188989899_张三``；\n",
    "+ 提交方式：1班发送至邮箱：632994085@qq.com；2班发送至邮箱：786888939@qq.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1：用``urllib.requests``库发起一次``get``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"8\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e66e0be-2852de82da7567c84322c284\"\n",
      "  }, \n",
      "  \"origin\": \"117.136.103.15\", \n",
      "  \"url\": \"http://httpbin.org/get\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import request, parse #导入request和parse\n",
    "url = 'http://httpbin.org/get' #输入要爬取的网页\n",
    "dict = {\n",
    "    'name': 'Hxh'\n",
    "}\n",
    "\n",
    "#设置urllib.request.Request中data，headers以及method\n",
    "\n",
    "#通过bytes(urllib.parse.urlencode())可以将post数据进行转换放到urllib.request.urlopen的data参数中\n",
    "data = bytes(parse.urlencode(dict), encoding='utf8')\n",
    "req = request.Request(url=url, data=data, method='GET')\n",
    "req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')\n",
    "\n",
    "#用urlopen()方法可以实现最基本请求的发起\n",
    "response = request.urlopen(req)\n",
    "#读取并打印相应内容，并将相应内容转化为utf-8格式\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"args\": {}, \n",
    "  \"headers\": { \n",
    "    \"Accept-Encoding\": \"identity\",  #浏览器发给服务器,声明浏览器支持的编码类型\n",
    "    \"Content-Length\": \"8\",  #发送的内容的长度 \n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",  #发送给服务器的格式\n",
    "    \"Host\": \"httpbin.org\", \n",
    "    #\"User-Agent\"用户代理,简称 UA,它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等。\n",
    "    \"User-Agent\": \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\", \n",
    "    #当应用程序负载均衡器处理请求时，跟踪信息将添加到 X-Amzn-Trace-Id 标头\n",
    "    \"X-Amzn-Trace-Id\": \"Root=1-5e66e0be-2852de82da7567c84322c284\"\n",
    "  }, \n",
    "  \"origin\": \"117.136.103.15\",  #IP地址\n",
    "  \"url\": \"http://httpbin.org/get\" #请求访问的网址\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库伪装成火狐浏览器发起一次``post``请求，输出``response``文本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"name\": \"Hxh\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"8\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/4.0(compatible;MSIE 5.5;Windows NT)\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-5e66da8b-f3c1e484efdc9fc034f0a730\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"117.136.103.15\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这里编写代码\n",
    "import requests\n",
    "from urllib import parse\n",
    "url = 'http://httpbin.org/post'\n",
    "headers = {\n",
    "    #伪装一个火狐浏览器\n",
    "    'User-Agent':'Mozilla/4.0(compatible;MSIE 5.5;Windows NT)',\n",
    "    'host':'httpbin.org'\n",
    "}\n",
    "dict = {\n",
    "    'name':'Hxh'\n",
    "}\n",
    "\n",
    "#设置urllib.request.Request中data，headers以及method\n",
    "\n",
    "#通过bytes(urllib.parse.urlencode())可以将post数据进行转换放到urllib.request.urlopen的data参数中\n",
    "data = bytes(parse.urlencode(dict),encoding = 'utf8')\n",
    "req = request.Request(url = url,data = data,headers = headers,method = 'POST')\n",
    "\n",
    "#用urlopen()方法可以实现最基本请求的发起\n",
    "response = request.urlopen(req)\n",
    "#读取并打印相应内容，并将相应内容转化为utf-8格式\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"args\": {}, \n",
    "  \"data\": \"\", \n",
    "  \"files\": {}, \n",
    "  \"form\": {\n",
    "    \"name\": \"Hxh\"\n",
    "  }, \n",
    "  \"headers\": {\n",
    "    \"Accept-Encoding\": \"identity\",  #浏览器发给服务器,声明浏览器支持的编码类型\n",
    "    \"Content-Length\": \"8\", #发送的内容的长度 \n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",  #发送给服务器的格式\n",
    "    \"Host\": \"httpbin.org\", \n",
    "    #\"User-Agent\"用户代理,简称 UA,它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等。\n",
    "    \"User-Agent\": \"Mozilla/4.0(compatible;MSIE 5.5;Windows NT)\", \n",
    "    #当应用程序负载均衡器处理请求时，跟踪信息将添加到 X-Amzn-Trace-Id 标头\n",
    "    \"X-Amzn-Trace-Id\": \"Root=1-5e66da8b-f3c1e484efdc9fc034f0a730\"\n",
    "  }, \n",
    "  \"json\": null, \n",
    "  \"origin\": \"117.136.103.15\",   #IP地址\n",
    "  \"url\": \"http://httpbin.org/post\" #请求访问的网址\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用``Requests``库中的``session``对象发出``get``请求，设置``cookies``并获取，输出获取的``cookies``内容。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cookies\": {\n",
      "    \"sessioncookie\": \"13459\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这里编写代码\n",
    "\n",
    "# 创建一个session对象 \n",
    "s = requests.Session()\n",
    "# 用session对象发出get请求，设置cookies \n",
    "s.get('http://httpbin.org/cookies/set/sessioncookie/13459')\n",
    "# 用session对象发出另外一个get请求，获取cookies \n",
    "r = s.get('http://httpbin.org/cookies')\n",
    "# 显示结果 \n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"cookies\": {\n",
    "    \"sessioncookie\": \"13459\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现抓取网页的不去重深度遍历算法，自选合适的种子网站和相关参数，输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "\n",
    "import requests, re,queue\n",
    "count = 10\n",
    "#给定正则化的格式\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)') \n",
    "seed = 'http://httpbin.org/' #给定抓取的网页\n",
    "stack = [seed]\n",
    "storage = {}\n",
    "while len(stack) > 0 and count > 0:\n",
    "    try:\n",
    "        url = stack.pop(-1) #队头出队\n",
    "        html = requests.get(url).text #得到url的文本信息\n",
    "        #正则表达式re findall方法以列表的形式返回能匹配的子串\n",
    "        new_urls = r.findall(html)\n",
    "        stack.extend(new_urls) #将第一层中的url入队\n",
    "        storage[url] = len(new_urls)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写抓取网页的广度遍历算法（含去重和不去重），自选合适的种子网站和相关参数，输出结果。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "\n",
    "#不去重\n",
    "import requests, re\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)') #给定正则化的格式\n",
    "seed = 'http://httpbin.org/' #给定抓取的网页\n",
    "queue = [seed]\n",
    "storage = {}\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = queue.pop(0) #队头出队\n",
    "        html = requests.get(url).text  #得到url的文本信息\n",
    "        #正则表达式re findall方法以列表的形式返回能匹配的子串\n",
    "        new_urls = r.findall(html)\n",
    "        queue.extend(new_urls) #将第一层中的url入队\n",
    "        storage[url] = len(new_urls)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去重\n",
    "import requests,re\n",
    "r=re.compile(r'href=[\\'\"]?(http[^\\'\">]+)') #给定正则化的格式\n",
    "seed=\"http://httpbin.org/\"  #给定抓取的网页\n",
    "queue=[seed]\n",
    "count=3\n",
    "storage={}  #暂时存储某一层下的url\n",
    "used=set([])  #用于存储已经爬取过的url\n",
    "while count>0 and len(queue)>0:\n",
    "    url=queue.pop(0)  \n",
    "    html=requests.get(url).text  #得到url的文本信息\n",
    "    storage[url]=html\n",
    "    used.add(url)\n",
    "    new_urls=r.findall(html)  #正则表达式re findall方法以列表的形式返回能匹配的子串\n",
    "    for new_url in new_urls:  \n",
    "        if new_url not in used and new_url not in queue:\n",
    "            print(new_url)\n",
    "            queue.append(new_url)\n",
    "    count-=count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (选)将抓取网页的去重深度遍历算法封装成对象（类），并测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里编写代码\n",
    "import requests, re,queue\n",
    "count = 10\n",
    "r = re.compile(r'href=[\\'\"]?(http[^\\'\" >]+)') #给定正则化的格式\n",
    "seed = 'http://httpbin.org/' #给定抓取的网页\n",
    "queue = [seed]\n",
    "#暂时存储某一层下的url\n",
    "storage = {}\n",
    "#存储已经爬取过的url\n",
    "used = set([]) #创建一个空集合\n",
    "while len(queue) > 0 and count > 0:\n",
    "    try:\n",
    "        url = queue.pop(0) #队头出队，进行深度遍历\n",
    "        html = requests.get(url).text #得到该url的文本信息\n",
    "        storage[url] = html\n",
    "        used.add(url) #将已经抓取过的URL存入used集合中\n",
    "        #正则表达式re findall方法以列表的形式返回能匹配的子串\n",
    "        new_urls = r.findall(html)\n",
    "        #将新发新未抓取的URL添加到queue中\n",
    "        for new_url in new_urls:\n",
    "            if new_url not in used and new_url not in queue:\n",
    "                queue.append(new_url)\n",
    "        count -= 1\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里对结果进行分析。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
